{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9479fa",
   "metadata": {},
   "source": [
    "## Classical error correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63a141",
   "metadata": {},
   "source": [
    "Information is an inherently physical object. To communicate it we must attach it to a physical manifestation such as speech, writing or actions. Any physical system can fail and when it does the information it carries can be lost or distorted.\n",
    "\n",
    "Suppose Alice and Bob agree that Alice will turn the kitchen light on if she is working from home and turn it off otherwise. The idea is efficient but it depends entirely on the light functioning. If the bulb fails, Bob may draw the wrong conclusion despite Aliceâ€™s intention remaining unchanged. To reduce this risk Alice could instead turn on the kitchen, bedroom and bathroom lights. It is unlikely that all three bulbs fail at once, so Bob is far more likely to receive the intended message.\n",
    "\n",
    "This is an example of classical error correction. Using multiple bulbs introduces redundancy into the communication channel so that even when parts of the physical system fail the original message can still be recovered. The goal of error correction is to introduce redundancy and structure into the communication channel so that the information remains intact despite physical imperfections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b05dcb",
   "metadata": {},
   "source": [
    "### Information as binary strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b559ec5",
   "metadata": {},
   "source": [
    "Often we can break the information we wish to communicate down into yes/no questions. In the light switch example, the switch is either on or off. We can encode these two possibilities as $1$ and $0$, where $1$ is on and $0$ is off. This is an example of a _bit_. A bit represents a physical system with two possible states which we label $0$ and $1$. If Alice wants to indicate she if working from home she sends a $1$ and otherwise she sends a $0$. \n",
    "\n",
    "An error occurs when the bit changes physical state; that is, a $0$ becomes a $1$ or a $1$ becomes a $0$. These are called _bit-flip errors_. For instance, if Alice tries to send a $1$ but the light switch fails, Bob will receive a $0$ and incorrectly interpret the message.\n",
    "\n",
    "To reduce this risk we add redundancy to the system. In the earlier example with three switches Alice sends a $0$ or a $1$ for each switch; or equivalently, the binary string $000$ or $111$. However, due to physical imperfections in the communication channel, any of these bits can flip and Bob could receive any of the $2^8$ possible triples of 0's and 1's. For example, if Alice sends $111$ but the middle switch fails, then Bob will receive the message $101$. Since two of the three bits are 1's, Bob can deduce it is more likely that only one error occurred and the intended message was $111$.\n",
    "\n",
    "Binary strings of length $n$ are vectors over $(\\mathbb{Z}/2\\mathbb{Z})^n$. In this example, Alice sends a vector from the vector subspace $\\{(0,0,0), (1,1,1)\\}$. However, due to errors, Bob could receive any vector in $(\\mathbb{Z}/2\\mathbb{Z})^3$. This idea can be generalised by choosing suitable subspaces of $(\\mathbb{Z}/2\\mathbb{Z})^n$ to define _error-correcting codes_, that allow both the detection and correction of errors in transmitted messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29305573",
   "metadata": {},
   "source": [
    "#### Example. \n",
    "Before moving on to general error correction codes we consider an implementation of the light switch example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42dac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def send_bit(bit, error_prob=0.2):\n",
    "    \"\"\" \n",
    "    Sends one bit across a noisy channel.\n",
    "    Each bit has probability error_prob at being flipped.\n",
    "    \"\"\"\n",
    "    if random.random() < error_prob:\n",
    "        return 1 - bit\n",
    "    return bit\n",
    "\n",
    "print(f\"\\nSingle light switch\")\n",
    "alice_intended = random.choice([0,1])\n",
    "bob_interpretation = send_bit(alice_intended)\n",
    "print(f\"Alice sent: {alice_intended}, Bob received: {bob_interpretation}\")\n",
    "\n",
    "print(f\"\\nThree light switches\")\n",
    "alice_intended = random.choice([0,1])\n",
    "alice_message = [alice_intended]*3\n",
    "bob_message = [send_bit(bit) for bit in alice_message]\n",
    "print(f\"Alice sent: {alice_message}, Bob received: {bob_message}\")\n",
    "\n",
    "def decode_message(bits):\n",
    "    return 1 if sum(bits) > len(bits)//2 else 0\n",
    "\n",
    "bob_interpretation = decode_message(bob_message)\n",
    "print(f\"Bob decodes message as: {bob_interpretation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddd725",
   "metadata": {},
   "source": [
    "As the number of bits (number of light switches) increases the likelihood that Bob interprets the correct message increases, but the time taken to decode also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "error_prob = 0.2 \n",
    "num_trials = 500000\n",
    "num_bits = range(1, 31, 2)\n",
    "success_probs = []\n",
    "decode_times = []\n",
    "\n",
    "for n in num_bits:\n",
    "    successes = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        alice_intention = random.choice([0,1])\n",
    "        alice_message = [alice_intention]*n\n",
    "        bob_message = [send_bit(i, error_prob) for i in alice_message]\n",
    "        bob_interpretation = decode_message(bob_message)\n",
    "        trial_time = time.time() - start_time\n",
    "        total_time += trial_time\n",
    "        if bob_interpretation == alice_intention:\n",
    "            successes += 1\n",
    "\n",
    "    success_prob = float(successes/num_trials)\n",
    "    success_probs.append(success_prob)\n",
    "\n",
    "    decode_time = float(total_time/num_trials)\n",
    "    decode_times.append(decode_time)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "fig.suptitle(\"Majority vote decoding vs number of bits\", fontsize=12)\n",
    "\n",
    "ax1.plot(num_bits, success_probs)\n",
    "ax1.set(\n",
    "    ylabel=\"Probability of correct decoding\",\n",
    "    #title=f\"Probability correct message is decoded using majority vote vs number of encoding bits. \\n Error prob = {error_prob}.\"\n",
    "    )\n",
    "ax1.grid(visible=True)\n",
    "\n",
    "ax2.plot(num_bits, decode_times)\n",
    "ax2.set(\n",
    "    xlabel=\"Number of bits used to encode message\",\n",
    "    ylabel=\"Time taken to decode\",\n",
    "    #title=f\"Time taken to decode message using majority vote vs number of encoding bits. \\n Error prob = {error_prob}.\"\n",
    "    )\n",
    "ax2.grid(visible=True)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cb41c",
   "metadata": {},
   "source": [
    "Ideally we want Bob receives the message Alice intended with high probability whilst using as few resources as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90022d4f",
   "metadata": {},
   "source": [
    "### Error correction codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65200532",
   "metadata": {},
   "source": [
    "Fix an integer $n \\in \\mathbb{Z}_{>0}$. A _(binary) linear code_ of _length_ $n$ is a vector subspace $$\\mathcal{C} \\subseteq (\\mathbb{Z}/2\\mathbb{Z})^n.$$ An element of the code is called a _codeword_. The _rank_ of a code is its dimension as a vector space over $\\mathbb{Z}/2\\mathbb{Z}$.\n",
    "\n",
    "#### Examples.\n",
    "\n",
    "1. The set $\\mathcal{C}_1 = \\{(0,0,0), (1,1,1)\\}$ is a vector subspace of $(\\mathbb{Z}/2\\mathbb{Z})^3$, and hence a code. The basis of the code is $\\{(1,1,1)\\}$. Thus, $\\mathcal{C}_1$ is a code of length $3$ and rank $1$ that contains $2$ codewords.\n",
    "\n",
    "2. The set\n",
    "$$\\mathcal{C}_2 = \\{(0,0,0),(1,1,0),(1,0,1),(0,1,1)\\}$$\n",
    "also forms a subspace of $(\\mathbb{Z}/2\\mathbb{Z})^3$, and so is a code. One basis of $\\mathcal{C}_2$ is $\\{(1,1,0),(1,0,1)\\}$. Thus, $\\mathcal{C}_2$ is a code of length $3$ and rank $2$ that contains $4$ codewords.\n",
    "\n",
    "3. For any subset $S$ of $(\\mathbb{Z}/2\\mathbb{Z})^n$ the span\n",
    "$$\\mathcal{C}(S) = \\mathrm{Span}_{\\mathbb{Z}/2\\mathbb{Z}}(S)$$\n",
    "defines a binary code of length $n$. The rank and number of codewords of $\\mathcal{C}(S)$ depends on the linear relations between the elements of $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed896c7",
   "metadata": {},
   "source": [
    "Since a code is a vector subspace, a code is uniquely determined by a basis. A _generator matrix_ of a code is a matrix whose rows form a basis of the code. Note that a basis of a vector space is not unique and hence the generator matrix of a code is not unique either.\n",
    "\n",
    "#### Examples.\n",
    "\n",
    "1. One generator matrix for $\\mathcal{C}_1$ is\n",
    "$$ \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}. $$\n",
    "\n",
    "2. One generator matrix for $\\mathcal{C}_2$ is\n",
    "$$ \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0729f385",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class LinearCode without an implementation for abstract method 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlinearcodes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearCode\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# C1 = {000,111}\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m M1 = \u001b[43mLinearCode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mC1 has codewords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM1.elements\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM1.length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Can't instantiate abstract class LinearCode without an implementation for abstract method 'decode'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from linearcodes import LinearCode\n",
    "\n",
    "# C1 = {000,111}\n",
    "\n",
    "M1 = LinearCode([[0,0,0],[1,1,1]])\n",
    "print(f\"\\nC1 has codewords: {M1.elements};\")\n",
    "print(f\"  Length: {M1.length};\")\n",
    "print(f\"  Rank: {M1.rank};\")\n",
    "G1 = M1.generator_matrix\n",
    "print(f\"  Generator matrix: \\n{G1.array}.\")\n",
    "\n",
    "# C2 = {000,110,101,011}\n",
    "\n",
    "M2 = LinearCode([[0,0,0],[1,1,0],[1,0,1],[0,1,1]])\n",
    "print(f\"\\nC2 has codewords: {M2.elements};\")\n",
    "print(f\"  Length: {M2.length};\")\n",
    "print(f\"  Rank: {M2.rank};\")\n",
    "G2 = M2.generator_matrix\n",
    "print(f\"  Generator matrix: \\n{G2.array}.\")\n",
    "\n",
    "# A random code C3\n",
    "\n",
    "num_gen_elts = 3\n",
    "binary_dimension = 5\n",
    "elts = []\n",
    "for elt in range(num_gen_elts):\n",
    "    elt = []\n",
    "    for bit in range(binary_dimension):\n",
    "        elt.append(random.choice([0,1]))\n",
    "    elts.append(elt)\n",
    "\n",
    "M3 = LinearCode(elts)\n",
    "codewords = M3.elements\n",
    "print(f\"\\nC3 has {len(codewords)} codewords:\")\n",
    "print(f\"  {codewords};\")\n",
    "print(f\"  Length: {M3.length};\")\n",
    "print(f\"  Rank: {M3.rank};\")\n",
    "G3 = M3.generator_matrix\n",
    "print(f\"  Generator matrix: \\n{G3.array}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816b043",
   "metadata": {},
   "source": [
    "#### Parity checks\n",
    "\n",
    "A basis of a code $\\mathcal{C}$ can be represented by a matrix $B$ whose rows are precisely the basis vectors. Suppose $\\mathcal{C}$ is a code of length $n \\in \\mathbb{Z}_{>0}$ and rank $k \\leq n$. The matrix $B$ defines a basis change map $B \\colon (\\mathbb{Z}/2\\mathbb{Z}^n) \\to \\mathcal{C}$. Since this basis change map is surjective its kernel has rank $n-k$ by the Rank-Nullity Theorem. Let $P$ be a matrix for the basis of the kernel. This is called the _parity check matrix_. We can use the parity check matrix to see whether a message lies in the codespace. In particular, for a message $m$ we have that $m$ is a codeword if and only if $Pm = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abcf1d",
   "metadata": {},
   "source": [
    "#### Tanner Graph\n",
    "\n",
    "The _Tanner graph_ of the code is a bipartite graph whose vertex sets are:\n",
    "1. The set of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc29e6",
   "metadata": {},
   "source": [
    "### Error correction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5da90b",
   "metadata": {},
   "source": [
    "Linear codes are a classical technique for error correction in message transmission. They work by adding redundancy to messages so that errors can be detected and corrected. We have already seen an example of this with the code \n",
    "$$\n",
    "\\mathcal{C}_1 = \\{000, 111\\}.\n",
    "$$\n",
    "\n",
    "##### Encoding\n",
    "\n",
    "Recall that Alice wants to tell Bob whether she is working. Alice could send Bob a single bit of information, either $0$ or $1$, but in this case, any error during transmission could cause Bob to misinterpret the message. To reduce this risk, Alice instead sends three identical bits: $000$ for $0$ and $111$ for $1$. Formally, she encodes the message space \n",
    "$$\n",
    "\\mathcal{M} = \\{0,1\\}\n",
    "$$ \n",
    "as codewords of $\\mathcal{C}_1$ using a bijective map\n",
    "$$\n",
    "E \\colon \\mathcal{M} \\to \\mathcal{C}_1\n",
    "$$\n",
    "where $E(0) = 000$ and $E(1) = 111$, which is shared with Bob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a7380",
   "metadata": {},
   "source": [
    "### Maximum likelihood decoder\n",
    "\n",
    "When a codeword is sent through a physical channel, some bits may be corrupted. As a result, the received message might differ from the original codeword. Decoding aims to recover the original codeword despite these errors. Without knowing the exact errors that occurred, we cannot be certain that decoding will find the intended message. So instead we try to find the codeword that is most likely to have been sent. More precisely, let $\\mathcal{C}$ be a code of length $n \\in \\mathbb{Z}_{>0}$ and suppose we receive a message $m \\in \\{0,1\\}^n$. Then we wish to find a codeword $\\hat{c} \\in \\mathcal{C}$ such that \n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(\\text{ Sent } \\hat{c} \\mid \\text{ Received } m) = \\mathrm{max}_{c \\in \\mathcal{C}} \\{\\mathbb{P}(\\text{ Sent } c \\mid \\text{ Received } m)\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "We denote $\\mathbb{P}(\\text{ Sent } c \\mid \\text{ Received } m)$ by  $\\mathbb{P}(c \\mid m)$. \n",
    "The application of Bayes Theorem to $\\mathbb{P}(c \\mid m)$ results in\n",
    "$$\\mathbb{P}(c \\mid m) = \\frac{\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)}{\\mathbb{P}(m)}.$$\n",
    "Notice that $\\mathbb{P}(m)$ does not depend on the specific codeword $c$, and so, to maximise $\\mathbb{P}(c \\mid m)$ over the code $\\mathcal{C}$ it suffices to maximise $$\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)$$ over $\\mathcal{C}$. To do this, we first make a couple of assumptions:\n",
    "\n",
    "1. **Uniform codeword distribution:** Each codeword is equally likely to be sent. That is, for all codewords $c \\in \\mathcal{C}$ we have that $$\\mathbb{P}(c) = \\frac{1}{|\\mathcal{C}|}.$$ Thus, maximising $\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)$ over $\\mathcal{C}$ is equivalent to maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$.\n",
    "\n",
    "2. **All errors are bit-flips:** A codeword $c \\in \\mathcal{C}$ produces a message $m$ that is a binary string of the same length. \n",
    "\n",
    "3. **Bit-flips are independent:** Each bit flips independently of others. That is, for a message $m=m_1m_2\\dots m_n$ and a codeword $c=c_1c_2\\dots c_n$, we have that\n",
    "    $$\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i).$$\n",
    "    In particular, since both $m_i$ and $c_i$ are bits, we have that:\n",
    "    $$ \\mathbb{P}(m_i \\mid c_i) = \\begin{cases} \\mathbb{P}(c_i \\text{ not flipped}) &\\text{ if } m_i=c_i \\\\ \\mathbb{P}(c_i \\text{ flipped to } m_i) &\\text{ if } m_i \\neq c_i. \\end{cases} $$\n",
    "\n",
    "Therefore, to compute $\\mathbb{P}(c \\mid m)$ for any codeword $c$ and message $m$ of length $n \\in \\mathbb{Z}_{>0}$, it suffices to know for each $1 \\leq i \\leq n$, the probabilities\n",
    "$$ \\mathbb{P}(c_i \\text{ flips } 0 \\to 1) \\; \\text{ and } \\; \\mathbb{P}(c_i \\text{ flips } 1 \\to 0). $$\n",
    "A _binary symmetric channel_ is a channel where these two probabilities coincide and a bit flips with the same probability from $0$ to $1$ as it does from $1$ to $0$.\n",
    "\n",
    "---\n",
    "##### Example.\n",
    "\n",
    "Let us consider the code $\\mathcal{C} = \\{000, 111\\}$. Suppose that we use a binary symmetric channel and each bit has a probability $p=\\frac{1}{4}$ of flipping regardless of position. Suppose that we receive the message $m=100$ and we wish to compute the most likely codeword it came from. Since maximising $\\mathbb{P}(c \\mid m)$ over $\\mathcal{C}$ is equivalent to maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$, it suffices to compute $\\mathbb{P}(m \\mid c)$ for each codeword $c \\in \\mathcal{C}$. That is, for $c=000$ and $c=111$.\n",
    "\n",
    "First suppose the codeword $c$ was $000$. Then receiving the message $m=100$ means the first bit flipped and the other two did not flip (TODO: can a bit flip more than once?). In particular, we have\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}(m=100 \\mid c=000) &= \\mathbb{P}(m_1=1 \\mid c_1=0) \\cdot \\mathbb{P}(m_2=0 \\mid c_2=0) \\cdot \\mathbb{P}(m_3=0 \\mid c_3=0) \\\\ &= \\mathbb{P}(c_1 \\text{ did flip}) \\cdot \\mathbb{P}(c_2 \\text{ did not flip}) \\cdot \\mathbb{P}(c_3 \\text{ did not flip}) \\\\ &= \\frac{1}{4} \\cdot \\frac{3}{4} \\cdot \\frac{3}{4} = \\frac{9}{64}. \n",
    "\\end{align*}\n",
    "Now suppose the codeword $c$ was $111$, then receiving the message $m=100$ means the first bit did not flip and the other two did flip, so we have\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(m=100 \\mid c=111) = \\frac{3}{4} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} = \\frac{3}{64}. \n",
    "\\end{equation*}\n",
    "Therefore, as $\\mathbb{P}(m=100 \\mid c=000)$ is larger than $\\mathbb{P}(m=100 \\mid c=111)$ it is most likely that the message $m=100$ came from the codeword $c=000$.\n",
    "<!-- \n",
    "Therefore, we have that\n",
    "\\begin{align*}\n",
    "    \\sum_{b \\in \\mathcal{C}} \\mathbb{P}(m \\mid b) &= P(m \\mid c=000) + P(m \\mid c=111) \n",
    "    \\\\\n",
    "    &= \\frac{9}{64} + \\frac{3}{64} = \\frac{3}{16}.\n",
    "\\end{align*}\n",
    "Thus, substituting these probabilities in, it follows that\n",
    "$$\\mathbb{P}(c=000 \\mid m) = \\frac{1}{|\\mathcal{C}|} \\cdot \\frac{\\mathbb{P}(m \\mid c=000)}{\\sum_{d \\in \\mathcal{C}} \\mathbb{P}(m \\mid d)} =  \\frac{1}{1}\\cdot \\frac{\\frac{9}{64}}{\\frac{3}{16}} = \\frac{3}{4},$$\n",
    "and\n",
    "$$\\mathbb{P}(c=111 \\mid m) = \\frac{1}{|\\mathcal{C}|} \\cdot \\frac{\\mathbb{P}(m \\mid c=111)}{\\sum_{d \\in \\mathcal{C}} \\mathbb{P}(m \\mid d)} =  \\frac{1}{1}\\cdot \\frac{\\frac{3}{64}}{\\frac{3}{16}} = \\frac{1}{4}.$$\n",
    "Therefore, if $m=100$ is the transmitted message, then the most likely codeword that was sent is $c=000$ with a probability of $\\frac{3}{4}$. -->\n",
    "\n",
    "We can compute the most likely codeword for each of the possible messages $m \\in (\\mathbb{Z}/2\\mathbb{Z})^3$.\n",
    "\n",
    "<!-- | Message $m$   | $\\mathbb{P}(m \\mid c=000)$ | $\\mathbb{P}(m \\mid c=111)$ | $\\sum_{b \\in \\mathcal{C}} \\mathbb{P}(m \\mid b)$ | $\\mathbb{P}(c=000 \\mid m)$ | $\\mathbb{P}(c=111 \\mid m)$ |\n",
    "| ---                    | ---                    | ---                    | ---             |---                    | ---                    |\n",
    "| 000                    | $\\frac{27}{64}$        | $\\frac{1}{64}$         | $\\frac{7}{16}$  | $\\frac{27}{28}$        | $\\frac{1}{28}$         |\n",
    "| 100, 010, 001          | $\\frac{9}{64}$         | $\\frac{3}{64}$         | $\\frac{3}{16}$  | $\\frac{3}{4}$         | $\\frac{1}{4}$         |\n",
    "| 110, 101, 011          | $\\frac{3}{64}$         | $\\frac{9}{64}$         | $\\frac{3}{16}$  | $\\frac{1}{4}$         | $\\frac{3}{4}$         |\n",
    "| 111                    | $\\frac{1}{64}$         | $\\frac{27}{64}$        | $\\frac{7}{16}$  | $\\frac{1}{28}$         | $\\frac{27}{28}$        |  -->\n",
    "\n",
    "| Message $m$   | $\\mathbb{P}(m \\mid c=000)$ | $\\mathbb{P}(m \\mid c=111)$  | Most likely codeword $c$ |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | $\\frac{27}{64}$        | $\\frac{1}{64}$         | 000 <!-- with probability $\\frac{27}{28}$ --> |\n",
    "| 100, 010, 001          | $\\frac{9}{64}$         | $\\frac{3}{64}$         | 000 <!-- with probability $\\frac{3}{4}$ --> |\n",
    "| 110, 101, 011          | $\\frac{3}{64}$         | $\\frac{9}{64}$         | 111 <!-- with probability $\\frac{3}{4}$ --> |\n",
    "| 111                    | $\\frac{1}{64}$         | $\\frac{27}{64}$        | 111 <!-- with probability $\\frac{27}{28}$ --> |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be825a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Maximimum likelihood decoder 1 ===\n",
      "Decode correct? True\n",
      "Time to decode = 5.698x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 2 ===\n",
      "Decode correct? False\n",
      "Time to decode = 2.599x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 3 ===\n",
      "Decode correct? False\n",
      "Time to decode = 2.217x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 4 ===\n",
      "Decode correct? True\n",
      "Time to decode = 2.003x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 5 ===\n",
      "Decode correct? True\n",
      "Time to decode = 1.955x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 6 ===\n",
      "Decode correct? True\n",
      "Time to decode = 1.955x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 7 ===\n",
      "Decode correct? False\n",
      "Time to decode = 2.170x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 8 ===\n",
      "Decode correct? True\n",
      "Time to decode = 2.193x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 9 ===\n",
      "Decode correct? False\n",
      "Time to decode = 2.074x10^5\n",
      "\n",
      "=== Maximimum likelihood decoder 10 ===\n",
      "Decode correct? True\n",
      "Time to decode = 2.003x10^5\n"
     ]
    }
   ],
   "source": [
    "from linearcodes import Codeword, LinearCode, HammingCode\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def send_codeword(\n",
    "        code_length: int,\n",
    "        channel_probabilities: list[float],\n",
    "        codeword: Codeword):\n",
    "    if len(channel_probabilities) != code_length:\n",
    "        raise ValueError(f\"List of probabilities must be of length code_length={code_length}. Got {len(channel_probabilities)}.\")\n",
    "    if len(codeword) != code_length:\n",
    "        raise ValueError(f\"Codeword must of length code_length={code_length}. Got {len(codeword)}.\")\n",
    "    message = []\n",
    "    for bit_idx in range(code_length):\n",
    "        p = channel_probabilities[bit_idx]\n",
    "        c = codeword.vector[bit_idx]\n",
    "        r = np.random.uniform(0,1)\n",
    "        if r < p:\n",
    "            flip_bit = (c+1)%2\n",
    "            message.append(flip_bit)\n",
    "        else:\n",
    "            message.append(c)\n",
    "    return message\n",
    "\n",
    "def maximum_likelihood_decoder(channel_probabilities: list[float],\n",
    "                               code: LinearCode, \n",
    "                               message: list[int]) -> Codeword:\n",
    "    \"\"\"\n",
    "    Returns the codeword that was most likely to be sent given message is received.\n",
    "    Args:\n",
    "        - channel_probabilities (list[float]):\n",
    "            entry with index i is the probability bit i will flip.\n",
    "        - code (lc.LinearCode):\n",
    "            code used for encoding.\n",
    "        - message (list[int]):\n",
    "            received message.\n",
    "    \"\"\"\n",
    "    n = code.length\n",
    "    if len(channel_probabilities) != n:\n",
    "        raise ValueError(f\"List of probabilities must be of length {n}. Got {len(channel_probabilities)}.\")\n",
    "    if len(message) != n:\n",
    "        raise ValueError(f\"Message must be of length {n}. Got {len(message)}.\")\n",
    "    codewords = code.codewords\n",
    "    most_likely_codeword = Codeword([0]*n)\n",
    "    most_likely_codeword_prob = 0\n",
    "    for c in codewords:       \n",
    "        c_probability = 1\n",
    "        for bit_idx in range(n):\n",
    "            p = channel_probabilities[bit_idx]\n",
    "            if c.vector[bit_idx] != message[bit_idx]:\n",
    "                c_probability *= p\n",
    "            else:\n",
    "                c_probability *= (1-p)\n",
    "        if c_probability > most_likely_codeword_prob:\n",
    "            most_likely_codeword = c\n",
    "            most_likely_codeword_prob = c_probability\n",
    "    return most_likely_codeword\n",
    "\n",
    "code = HammingCode(3)\n",
    "code_length = code.length\n",
    "codewords = code.codewords\n",
    "channel_probabilities = [float(0.1)]*code_length\n",
    "results: dict[int, tuple[bool, float]] = {}\n",
    "\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "    print(f\"\\n=== Maximimum likelihood decoder {trial+1} ===\")\n",
    "    codeword = random.choice(codewords)\n",
    "\n",
    "    message = send_codeword(code_length, channel_probabilities, codeword)\n",
    "    start_time = time.time()\n",
    "    most_likely_codeword = maximum_likelihood_decoder(channel_probabilities, code, message)\n",
    "    decode_time = time.time() - start_time\n",
    "\n",
    "    # print(f\"\\nRows are: sent codeword c, received message m, most likely sent codeword c'.\")\n",
    "    data = np.array([codeword.vector, message, most_likely_codeword.vector])\n",
    "    # print(data)\n",
    "\n",
    "    decode_correct = codeword == most_likely_codeword\n",
    "    # print(f\"\\nDecoding correct? {decode_correct}\")\n",
    "    # print(f\"Actual number of errors: {sum(1 for bit_idx in range(code_length) if codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "    # print(f\"Guessed number of errors: {sum(1 for bit_idx in range(code_length) if most_likely_codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "\n",
    "    results[trial] = (decode_correct, decode_time)\n",
    "    print(f\"Decode correct? {decode_correct}\")\n",
    "    print(f\"Time to decode = {decode_time*(10**5):.3f}x10^5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e096ded",
   "metadata": {},
   "source": [
    "\n",
    "#### Hamming distance\n",
    "\n",
    "We can simplify the Maximum Likelihood Decoder by using the Hamming distance. Recall maximising $\\mathbb{P}(c \\mid m)$ is equivalent to maximising $\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i)$. Let us assume we have a binary symmetric channel such that the probability the probability a bit flips is $p$. Therefore, we have that\n",
    "$$\n",
    "\\mathbb{P}(m_i \\mid c_i) =\n",
    "\\begin{cases}\n",
    "p & \\text{if } m_i \\neq c_i, \\\\\n",
    "1-p & \\text{if } m_i = c_i.\n",
    "\\end{cases}\n",
    "$$\n",
    "Notice that the probabilities depend only whether the bits are the same or different. In particular, the power of $p$ in the product $\\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i)$ is precisely the number of bits in $m$ that differ from the bits in $c$, that is the Hamming distance $d(m,c)$. Therefore, we have that \n",
    "$$\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i) = p^{d(m,c)}(1-p)^{n-d(m,c)}.$$\n",
    "Rearranging this expression we obtain\n",
    "$$\\mathbb{P}(m \\mid c) = \\left( \\frac{p}{1-p}\\right)^{d(m,c)}  \\cdot (1-p)^{n}.$$\n",
    "Note that $(1-p)^{n}$ depends neither on the codeword $c$ nor the message $m$. Thus, maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$ is equivalent to maximising\n",
    "$$\\left( \\frac{p}{1-p}\\right)^{d(m,c)}$$\n",
    "over $\\mathcal{C}$.\n",
    "\n",
    "By convention we assume that $p < \\frac{1}{2}$. If it is not, then we can assume the received message is more likely to be wrong and flip all the bits before working with them .... (Fix this phrasing!). Consequently, the value $\\frac{p}{1-p}$ is less than $1$ and so to maximise $$\\left( \\frac{p}{1-p}\\right)^{d(m,c)}$$ we must minimise $d(m,c)$. In particular, we have that the codeword $\\hat{c}$ that maximises $\\mathbb{P}(c \\mid m)$ is precisely the codeword that minimises $d(m,c)$.\n",
    "\n",
    "---\n",
    "#### Example.\n",
    "\n",
    "Let us consider the example $\\mathcal{C} = \\{000,111\\}$ again. Suppose we receive the message $m=100$. We know from previous computations that the most likely codeword that was sent is $c=000$. We can recompute this using the Hamming distance. In particular, we have that\n",
    "$$ d(m=100,c=000) = 1 \\; \\text{ and } \\; d(m=100,c=111) = 2 $$\n",
    "and so $c=000$ minimises the Hamming distance.\n",
    "\n",
    "As before we can compute the Hamming distance and most likely codeword for all possible messages $m \\in \\{0,1\\}^3$.\n",
    "| Message $m$   | $d(m, c=000)$ | $d(m, c=111)$ | Most likely codeword $c$ |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | 0        | 3         | 000  |\n",
    "| 100, 010, 001          | 1         | 2         | 000  |\n",
    "| 110, 101, 011          | 2         | 1         | 111  |\n",
    "| 111                    | 3         | 0        | 111  |\n",
    "---\n",
    "\n",
    "<!-- \n",
    "\n",
    "Moreover, since the log function is monotonically increasing maximising $\\mathbb{P}(m \\mid c)$ is equivalent to maximising $\\mathrm{log}(\\mathbb{P}(m \\mid c)) $. (TODO: make this a more natural jump.) By applying the log function we obtain\n",
    "$$ \\mathrm{log}(\\mathbb{P}(m \\mid c)) = \\mathrm{log}(\\prod_i \\mathbb{P}(m_i \\mid c_i)) = \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)). $$\n",
    "Now recall that for a binary symmetric channel we assume that \n",
    "In particular, it depends only on when bits are the same and when they are not, similar to the Hamming distance. Thereofore, by using the Hamming distance between two codewords we obtain\n",
    "$$ \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)) = d(c,m)\\mathrm{log}(p) + (1-d(c,m))\\mathrm{log}(1-p). $$\n",
    "Simplifying this we obtain\n",
    "$$ \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)) =  d(c,m)\\mathrm{log}(\\frac{p}{1-p}) + \\mathrm{log}(1-p). $$\n",
    "Notice that $\\mathrm{log}(1-p)$ does not depend on $c$, so maximising $\\mathbb{P}(c \\mid m)$ is equivalent to maximising $$d(c,m)\\mathrm{log}(\\frac{p}{1-p})$$. Since we assume that $p < 0.5$, we have that $\\mathrm{log}(\\frac{p}{1-p})$ is negative. Thus, maximising over $d(c,m)\\mathrm{log}(\\frac{p}{1-p})$ is equivalent to minimsiing over $d(c,m)$. Therefore, we have that\n",
    "$$ \\mathrm{max}_{c \\in \\mathcal{C}} (\\mathbb{P}(c \\mid m)) = \\mathrm{min}_{c \\in \\mathcal{C}} (d(c,m)).$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Take the previous example with the code $\\mathcal{C} = \\{000,111\\}$. We can easily compute the Hamming distances between any message $m$ and codeword $c$ to obtain the following table:\n",
    "\n",
    "| Message $m$   | $d(m, c=000)$ | $d(m, c=111)$ | Closest = most likely codeword |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | 0        | 3         | 000  |\n",
    "| 100, 010, 001          | 1         | 2         | 000  |\n",
    "| 110, 101, 011          | 2         | 1         | 111  |\n",
    "| 111                    | 3         | 0        | 111  | \n",
    "\n",
    "---\n",
    "-->\n",
    "Using the Hamming distance makes it faster to apply the maximum likelihood decoder. However, for a code $\\mathcal{C}$ with rank $k$, it still requires computing and comparing $2^k$ different numbers. Therefore, as the code gets bigger the Maximum Likelihood Decoder becomes computationally impractical. We need more efficient algorithms that estimate the most likely codeword without having to compute the Hamming distances explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbd7c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Minimise Hamming distance decoder 1 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 0 1 0 0 1 0]\n",
      " [1 1 1 0 0 1 0]\n",
      " [1 0 1 0 0 1 0]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 12.827x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 2 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 0 0 0 1 1 1]\n",
      " [0 0 0 0 1 1 1]\n",
      " [0 0 0 0 1 1 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 0\n",
      "Guessed number of errors: 0\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.313x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 3 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 0 1 0 1 0 1]\n",
      " [1 0 1 0 1 1 1]\n",
      " [1 0 1 0 1 0 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.170x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 4 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 1 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 0\n",
      "Guessed number of errors: 0\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.146x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 5 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 1 0 0 1 1]\n",
      " [0 1 1 0 0 1 0]\n",
      " [0 1 1 0 0 1 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.217x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 6 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 1 0 1 0 0]\n",
      " [0 1 1 1 1 0 0]\n",
      " [0 1 1 0 1 0 0]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.575x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 7 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 0 1 0 1 0]\n",
      " [1 1 1 1 0 1 0]\n",
      " [1 1 1 1 0 0 0]]\n",
      "\n",
      "Decoding correct? False\n",
      "Actual number of errors: 2\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? False\n",
      "Time to decode = 2.480x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 8 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 0 0 0 1 1 1]\n",
      " [0 0 1 1 1 0 1]\n",
      " [0 0 1 1 0 0 1]]\n",
      "\n",
      "Decoding correct? False\n",
      "Actual number of errors: 3\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? False\n",
      "Time to decode = 2.837x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 9 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 0 1 0 1 0]\n",
      " [0 0 0 1 0 1 1]\n",
      " [1 0 0 1 0 1 1]]\n",
      "\n",
      "Decoding correct? False\n",
      "Actual number of errors: 2\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? False\n",
      "Time to decode = 1.836x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 10 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 0 0 1 1 0 0]\n",
      " [1 1 0 1 1 0 0]\n",
      " [1 0 0 1 1 0 0]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 1.812x10^5\n"
     ]
    }
   ],
   "source": [
    "from linearcodes import Codeword, LinearCode, HammingCode\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def send_codeword(\n",
    "        code_length: int,\n",
    "        channel_probabilities: list[float],\n",
    "        codeword: Codeword):\n",
    "    if len(channel_probabilities) != code_length:\n",
    "        raise ValueError(f\"List of probabilities must be of length code_length={code_length}. Got {len(channel_probabilities)}.\")\n",
    "    if len(codeword) != code_length:\n",
    "        raise ValueError(f\"Codeword must of length code_length={code_length}. Got {len(codeword)}.\")\n",
    "    message = []\n",
    "    for bit_idx in range(code_length):\n",
    "        p = channel_probabilities[bit_idx]\n",
    "        c = codeword.vector[bit_idx]\n",
    "        r = np.random.uniform(0,1)\n",
    "        if r < p:\n",
    "            flip_bit = (c+1)%2\n",
    "            message.append(flip_bit)\n",
    "        else:\n",
    "            message.append(c)\n",
    "    return message\n",
    "\n",
    "def hamming_distance(x: list[int], y: list[int]) -> int:\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both x and y must be the same length.\")\n",
    "    return sum([1 for bit_idx in range(len(x)) if x[bit_idx] != y[bit_idx]])\n",
    "\n",
    "\n",
    "def minimise_hamming_distance(code_length: int,\n",
    "                              codewords: list[Codeword],\n",
    "                              message: list[int]) -> Codeword:\n",
    "    \"\"\"\n",
    "    Returns the codeword that was most likely to be sent given message is received using Hamming distance.\n",
    "    Args:\n",
    "        - channel_probabilities (list[float]):\n",
    "            entry with index i is the probability bit i will flip.\n",
    "        - code (lc.LinearCode):\n",
    "            code used for encoding.\n",
    "        - message (list[int]):\n",
    "            received message.\n",
    "    \"\"\"\n",
    "    if len(message) != code_length:\n",
    "        raise ValueError(f\"Message must be of length {code_length}. Got {len(message)}.\")\n",
    "    most_likely_codeword = Codeword([0]*code_length)\n",
    "    min_hamming_dist = code_length\n",
    "    for c in codewords:       \n",
    "        hamming_dist = hamming_distance(c.vector, message)\n",
    "        if hamming_dist < min_hamming_dist:\n",
    "            most_likely_codeword = c\n",
    "            min_hamming_dist = hamming_dist\n",
    "    return most_likely_codeword\n",
    "\n",
    "code = HammingCode(3)\n",
    "code_length = code.length\n",
    "codewords = code.codewords\n",
    "channel_probabilities = [float(0.1)]*code_length\n",
    "results: dict[int, tuple[bool, float]] = {}\n",
    "\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "    print(f\"\\n=== Minimise Hamming distance decoder {trial+1} ===\")\n",
    "    codeword = random.choice(codewords)\n",
    "\n",
    "    message = send_codeword(code_length, channel_probabilities, codeword)\n",
    "    start_time = time.time()\n",
    "    most_likely_codeword = minimise_hamming_distance(code_length, codewords, message)\n",
    "    decode_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nRows are: sent codeword c, received message m, most likely sent codeword c'.\")\n",
    "    data = np.array([codeword.vector, message, most_likely_codeword.vector])\n",
    "    print(data)\n",
    "\n",
    "    decode_correct = codeword == most_likely_codeword\n",
    "    print(f\"\\nDecoding correct? {decode_correct}\")\n",
    "    print(f\"Actual number of errors: {sum(1 for bit_idx in range(code_length) if codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "    print(f\"Guessed number of errors: {sum(1 for bit_idx in range(code_length) if most_likely_codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "\n",
    "    results[trial] = (decode_correct, decode_time)\n",
    "    print(f\"\\nDecode correct? {decode_correct}\")\n",
    "    print(f\"Time to decode = {decode_time*(10**5):.3f}x10^5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ea8d3",
   "metadata": {},
   "source": [
    "#### Decoding with parity checks\n",
    "\n",
    "Recall a linear code $\\mathcal{C}$ has a parity check matrix $P$ such that for any vector $v$ in the ambient space of $\\mathcal{C}$ we have that $Pv = 0$ if and only if $v$ is a codeword. We can use the parity check matrix to simplify the maximal likelihood method above.\n",
    "\n",
    "Recall in an ideal world for a message $m$ we want to compute $\\mathbb{P}(c \\mid m)$ for all codewords $c \\in \\mathcal{C}$ to find the most likely intended message and decode $m$. Currrently this requires $2^k$ computations where $k$ is the rank of the code. However, we can reduce this drastically by instead considering whether each of the individual bits are correct. \n",
    "\n",
    "First let us assume that no column in the parity check matrix $P$ is all 0's. If this is the case then puncturing at this index results in a code $\\mathcal{C}'$ that has the same rank as $\\mathcal{C}$ but length one smaller ... TODO: prove this somewhere. Now we claim that \n",
    "\n",
    "$$  = \\mathbb{P}(c \\mid m) = \\mathbb{P}(c \\mid m \\text{ and } Pc = 0) = \\mathbb{P}(\\bigcap_{i=1}^n c_i \\mid m \\text{ and } Pc = 0) $$\n",
    "\n",
    "$$ \\mathbb{P}(c \\text{ and } m \\text{ and } Pc = 0) = \\mathbb{P}(c \\text{ and } \\bigcap_{i=1}^n m_i \\text{ and } Pc = 0) $$\n",
    "$$ = \\mathbb{P}(m_1 \\mid c \\text{ and } \\bigcap_{i=2}^n m_i \\text{ and } Pc = 0 )\\mathbb{P}(c \\text{ and } \\bigcap_{i=2}^n m_i \\text{ and } Pc = 0 ) $$\n",
    "\n",
    "$$ \\mathbb{P}(\\bigcap_{i=1}^n c_i \\text{ and } m \\text{ and } Pc = 0) = \\mathbb{P}(c_1 \\mid \\bigcap_{i=2}^n c_i \\text{ and } m \\text{ and } Pc = 0)\\mathbb{P}(\\bigcap_{i=2}^n c_i \\text{ and } m \\text{ and } Pc = 0) $$\n",
    "\n",
    "$$= (\\prod_{j=1}^{n-1} \\mathbb{P}(c_j \\mid \\bigcap_{i=j+1}^n c_i \\text{ and } m \\text{ and } Pc = 0)) \\cdot \\mathbb{P}(c_n \\text{ and } m \\text{ and } Pc = 0)$$\n",
    "\n",
    "$$ \\mathbb{P}(c_i \\mid m) = \\frac{}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae50b43",
   "metadata": {},
   "source": [
    "#### Distance of a code\n",
    "\n",
    "We have already seen that we cannot with 100% certainty determine the correct codeword for a message $m$. However, if we know the number of errors that occured during transmission sometimes we can. The _distance_ of a code is \n",
    "A priori it is not clear that there is a unique most likely codeword for a message $m$. In fact, in general there is not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d277e5",
   "metadata": {},
   "source": [
    "#### Repetition Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearcodes import RepetitionCode\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# C1 = {000,111}\n",
    "\n",
    "C1 = RepetitionCode(3)\n",
    "num_trials = 4\n",
    "probs = np.random.uniform(0.2,0.6,3).tolist()\n",
    "print(f\"Probabilities of bit-flip: {probs}\")\n",
    "message_space = C1.message_space()\n",
    "for n in range(num_trials):\n",
    "    message = random.choice(message_space)\n",
    "    encoded, received, decoded = C1.send_and_decode_message(message, probs, verbose=True)\n",
    "    print(f\"\\n=== New transmission ===\")\n",
    "    print(f\"Message to send: {message}.\")\n",
    "    print(f\"Encoded message: {encoded}.\")\n",
    "    print(f\"Received message: {received}.\")\n",
    "    print(f\"Decoded message: {decoded}.\")\n",
    "    print(f\"Message decoded successfully: {message == decoded}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "codelength = 21\n",
    "C1 = RepetitionCode(codelength)\n",
    "num_trials = 4\n",
    "probs = np.random.uniform(0.2,0.5,codelength).tolist()\n",
    "print(f\"Probabilities of bit-flip: {probs}\")\n",
    "message_space = C1.message_space()\n",
    "for n in range(num_trials):\n",
    "    message = random.choice(message_space)\n",
    "    encoded, received, decoded = C1.send_and_decode_message(message, probs, verbose=True)\n",
    "    print(f\"\\n=== New transmission ===\")\n",
    "    print(f\"Message to send: {message}.\")\n",
    "    print(f\"Encoded message: {encoded}.\")\n",
    "    print(f\"Received message: {received}.\")\n",
    "    print(f\"Decoded message: {decoded}.\")\n",
    "    print(f\"Message decoded successfully: {message == decoded}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d981902",
   "metadata": {},
   "source": [
    "#### Hamming Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba5b73",
   "metadata": {},
   "source": [
    "#### LDPC Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a990338",
   "metadata": {},
   "source": [
    "An LDPC code is a linear code whose parity check matrix is sparse, meaning each parity check equation involves only a few variables. This sparsity makes it efficient to check whether a codeword contains an error. The parity checks can be visualised using a Tanner graph, a bipartite graph in which one set of nodes represents the bits in a codeword and the other set represents the parity check equations. A bit node is connected to a check node if that bit appears in the corresponding parity check equation.\n",
    "\n",
    "To define an LDPC code, one specifies the code length (number of bits), the number of parity check equations, and the weight of each parity check (the number of bits involved in the equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9d882",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195dd49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e199c51",
   "metadata": {},
   "source": [
    "## Belief propagation\n",
    "\n",
    "Belief propagation is a decoder that estimates the likelihood of an error by using the Tanner graph of a code. At each stage the algorithm chooses a bit vertex and a parity check vertex that are connected and asks:\n",
    "    What is the probability that the sent bit is a 0 or a 1 given the other received bits in the parity check?\n",
    "\n",
    "This algorithm involves running over different bits and parity checks updating the probabilities the sent bit was $0$ or $1$. The algorithm is suited to Tanner graphs that are sparse, since then only a few bits influence each parity check equation. Therefore, this is a decoder often used for LDPC codes. The main difficulty is deciding which bit and parity check to use at each step. If the same nodes are used too often then the influence of the rest of the graph is lost and the estimates become less reliable.\n",
    "\n",
    "Assume the code has length $n$ and we receive a message $r_1 r_2 \\dots r_n$. Let us denote the message that was sent by $s_1 s_2 \\dots s_n$. We first calculate the probability that each of the received bits is correct based on the probabilities that there will be a bit-flip in the channel. Assume the channel is symmetric, so the bit-flip probability is the same whether the bit is $0$ or $1$. Let the flip probability be $p$. The bit $r$ is correct if and only if no flip occurs, so  \n",
    "$$\n",
    "\\mathbb{P}(r \\text{ is correct}) = 1 - p \\; \\text{ and } \\; \\mathbb{P}(r \\text{ is incorrect}) = p.\n",
    "$$\n",
    "Now a single step of the belief propagation algorithm goes as follows:\n",
    "1. Choose one received bit $r_i$.  \n",
    "2. Choose a parity check $P$ that involves $r_i$, that is  \n",
    "   $$\n",
    "   P=\\sum_{j\\in J} r_j\n",
    "   $$  \n",
    "   for some set $J$ with $i\\in J$.  \n",
    "3. Update the probability that $s_i$ is a $0$ or a $1$ in light of the requirement given that the parity check $P$ needs to be satisified. That is, we want to compute $$\\mathbb{P}(s_i = 0 \\mid P \\text{ is satisfied.})  \\; \\text{ and } \\; \\mathbb{P}(s_i = 1\\mid P \\text{ is satisfied.}).$$ The key observation is that $P$ is satisfied if and only if  $$r_1 = \\sum_{j \\neq 1, j\\in J} r_j.$$  This relation, together with the current probabilities for the bits $r_2,\\dots,r_n$, determines the updated probability for $r$.\n",
    "\n",
    "$$ \\mathbb{P}(s_1 = r_1 \\mid P) = \\frac{\\mathbb{P}(s_1 = r_1 \\text{ and } P)}{\\mathbb{P}(P)} $$\n",
    "If $|J| = 2$.\n",
    "$$ \\mathbb{P}(s_1 = r_1 \\mid s_1 = s_2 ) = \\frac{\\mathbb{P}(s_1 = r_1 \\text{ and } s_1 = s_2 )}{\\mathbb{P}(s_1 = s_2)} $$\n",
    "\n",
    "\n",
    "$C = \\{110, 111, 001, 000\\}$ the probability any bit is $0$ or $1$ is $50-50$; ollie's clever rank nullity argument.\n",
    "\n",
    "so $P = x_1 + x_2$, say $p_1,p_2,p_3 = 0.4, 0.8, 0.3$.\n",
    "Suppose we receive $r = 101$. \n",
    "What is the probability the $s_2$ is a $0$ and a $1$ given that we know:\n",
    "1. The parity check must be satisfied i.e. $s_1 + s_2 = 0$.\n",
    "2. The probability that $s_1 = r_1$ is $0.5$ and $s_3 = r_3$ is $0.7$ i.e. no flips.\n",
    "\n",
    "So what is the proababilities:\n",
    "$P(s_2 = 0| s_1 + s_2 = 0, r_1, r_2)$ $P(s_2 = 1| s_1 + s_2 = 0, r_1, r_2)$\n",
    "\n",
    "$$ \\mathbb{P}(s_2 = 0| s_1 + s_2 = 0, r_1, r_2) = \\frac{\\mathbb{P}(s_2=0 \\text{ and }  s_1 + s_2 = 0, r_1, r_2)}{\\mathbb{P}(s_1 + s_2 = 0, r_1, r_2)} $$\n",
    "\n",
    "\n",
    "\n",
    "$P(s_1 = s_2)$\n",
    "If $r_1 = r_2$ it is the probability neither $r_1$ nor $r_2$ were flipped or both $r_1$ and $r_2$ were flipped.\n",
    "$P(r_1 \\text{ flip})P(r_2 \\text{ flip}) = $ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
