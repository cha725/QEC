{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4f4683",
   "metadata": {},
   "source": [
    "### Maximum likelihood decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a7380",
   "metadata": {},
   "source": [
    "When a codeword is sent through a physical channel, some bits may be corrupted. As a result, the received message might differ from the original codeword. Decoding aims to recover the original codeword despite these errors. Without knowing the exact errors that occurred, we cannot be certain that decoding will find the intended message. So instead we try to find the codeword that is most likely to have been sent. More precisely, let $\\mathcal{C}$ be a code of length $n \\in \\mathbb{Z}_{>0}$ and suppose we receive a message $m \\in \\{0,1\\}^n$. Then we wish to find a codeword $\\hat{c} \\in \\mathcal{C}$ such that \n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(\\text{ Sent } \\hat{c} \\mid \\text{ Received } m) = \\mathrm{max}_{c \\in \\mathcal{C}} \\{\\mathbb{P}(\\text{ Sent } c \\mid \\text{ Received } m)\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "We denote $\\mathbb{P}(\\text{ Sent } c \\mid \\text{ Received } m)$ by  $\\mathbb{P}(c \\mid m)$. \n",
    "The application of Bayes Theorem to $\\mathbb{P}(c \\mid m)$ results in\n",
    "$$\\mathbb{P}(c \\mid m) = \\frac{\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)}{\\mathbb{P}(m)}.$$\n",
    "Notice that $\\mathbb{P}(m)$ does not depend on the specific codeword $c$, and so, to maximise $\\mathbb{P}(c \\mid m)$ over the code $\\mathcal{C}$ it suffices to maximise $$\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)$$ over $\\mathcal{C}$. To do this, we first make a couple of assumptions:\n",
    "\n",
    "1. **Uniform codeword distribution:** Each codeword is equally likely to be sent. That is, for all codewords $c \\in \\mathcal{C}$ we have that $$\\mathbb{P}(c) = \\frac{1}{|\\mathcal{C}|}.$$ Thus, maximising $\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)$ over $\\mathcal{C}$ is equivalent to maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$.\n",
    "\n",
    "2. **All errors are bit-flips:** A codeword $c \\in \\mathcal{C}$ produces a message $m$ that is a binary string of the same length. \n",
    "\n",
    "3. **Bit-flips are independent:** Each bit flips independently of others. That is, for a message $m=m_1m_2\\dots m_n$ and a codeword $c=c_1c_2\\dots c_n$, we have that\n",
    "    $$\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i).$$\n",
    "    In particular, since both $m_i$ and $c_i$ are bits, we have that:\n",
    "    $$ \\mathbb{P}(m_i \\mid c_i) = \\begin{cases} \\mathbb{P}(c_i \\text{ not flipped}) &\\text{ if } m_i=c_i \\\\ \\mathbb{P}(c_i \\text{ flipped to } m_i) &\\text{ if } m_i \\neq c_i. \\end{cases} $$\n",
    "\n",
    "Therefore, to compute $\\mathbb{P}(c \\mid m)$ for any codeword $c$ and message $m$ of length $n \\in \\mathbb{Z}_{>0}$, it suffices to know for each $1 \\leq i \\leq n$, the probabilities\n",
    "$$ \\mathbb{P}(c_i \\text{ flips } 0 \\to 1) \\; \\text{ and } \\; \\mathbb{P}(c_i \\text{ flips } 1 \\to 0). $$\n",
    "A _binary symmetric channel_ is a channel where these two probabilities coincide and a bit flips with the same probability from $0$ to $1$ as it does from $1$ to $0$.\n",
    "\n",
    "---\n",
    "##### Example.\n",
    "\n",
    "Let us consider the code $\\mathcal{C} = \\{000, 111\\}$. Suppose that we use a binary symmetric channel and each bit has a probability $p=\\frac{1}{4}$ of flipping regardless of position. Suppose that we receive the message $m=100$ and we wish to compute the most likely codeword it came from. Since maximising $\\mathbb{P}(c \\mid m)$ over $\\mathcal{C}$ is equivalent to maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$, it suffices to compute $\\mathbb{P}(m \\mid c)$ for each codeword $c \\in \\mathcal{C}$. That is, for $c=000$ and $c=111$.\n",
    "\n",
    "First suppose the codeword $c$ was $000$. Then receiving the message $m=100$ means the first bit flipped and the other two did not flip (TODO: can a bit flip more than once?). In particular, we have\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}(m=100 \\mid c=000) &= \\mathbb{P}(m_1=1 \\mid c_1=0) \\cdot \\mathbb{P}(m_2=0 \\mid c_2=0) \\cdot \\mathbb{P}(m_3=0 \\mid c_3=0) \\\\ &= \\mathbb{P}(c_1 \\text{ did flip}) \\cdot \\mathbb{P}(c_2 \\text{ did not flip}) \\cdot \\mathbb{P}(c_3 \\text{ did not flip}) \\\\ &= \\frac{1}{4} \\cdot \\frac{3}{4} \\cdot \\frac{3}{4} = \\frac{9}{64}. \n",
    "\\end{align*}\n",
    "Now suppose the codeword $c$ was $111$, then receiving the message $m=100$ means the first bit did not flip and the other two did flip, so we have\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(m=100 \\mid c=111) = \\frac{3}{4} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} = \\frac{3}{64}. \n",
    "\\end{equation*}\n",
    "Therefore, as $\\mathbb{P}(m=100 \\mid c=000)$ is larger than $\\mathbb{P}(m=100 \\mid c=111)$ it is most likely that the message $m=100$ came from the codeword $c=000$.\n",
    "<!-- \n",
    "Therefore, we have that\n",
    "\\begin{align*}\n",
    "    \\sum_{b \\in \\mathcal{C}} \\mathbb{P}(m \\mid b) &= P(m \\mid c=000) + P(m \\mid c=111) \n",
    "    \\\\\n",
    "    &= \\frac{9}{64} + \\frac{3}{64} = \\frac{3}{16}.\n",
    "\\end{align*}\n",
    "Thus, substituting these probabilities in, it follows that\n",
    "$$\\mathbb{P}(c=000 \\mid m) = \\frac{1}{|\\mathcal{C}|} \\cdot \\frac{\\mathbb{P}(m \\mid c=000)}{\\sum_{d \\in \\mathcal{C}} \\mathbb{P}(m \\mid d)} =  \\frac{1}{1}\\cdot \\frac{\\frac{9}{64}}{\\frac{3}{16}} = \\frac{3}{4},$$\n",
    "and\n",
    "$$\\mathbb{P}(c=111 \\mid m) = \\frac{1}{|\\mathcal{C}|} \\cdot \\frac{\\mathbb{P}(m \\mid c=111)}{\\sum_{d \\in \\mathcal{C}} \\mathbb{P}(m \\mid d)} =  \\frac{1}{1}\\cdot \\frac{\\frac{3}{64}}{\\frac{3}{16}} = \\frac{1}{4}.$$\n",
    "Therefore, if $m=100$ is the transmitted message, then the most likely codeword that was sent is $c=000$ with a probability of $\\frac{3}{4}$. -->\n",
    "\n",
    "We can compute the most likely codeword for each of the possible messages $m \\in (\\mathbb{Z}/2\\mathbb{Z})^3$.\n",
    "\n",
    "<!-- | Message $m$   | $\\mathbb{P}(m \\mid c=000)$ | $\\mathbb{P}(m \\mid c=111)$ | $\\sum_{b \\in \\mathcal{C}} \\mathbb{P}(m \\mid b)$ | $\\mathbb{P}(c=000 \\mid m)$ | $\\mathbb{P}(c=111 \\mid m)$ |\n",
    "| ---                    | ---                    | ---                    | ---             |---                    | ---                    |\n",
    "| 000                    | $\\frac{27}{64}$        | $\\frac{1}{64}$         | $\\frac{7}{16}$  | $\\frac{27}{28}$        | $\\frac{1}{28}$         |\n",
    "| 100, 010, 001          | $\\frac{9}{64}$         | $\\frac{3}{64}$         | $\\frac{3}{16}$  | $\\frac{3}{4}$         | $\\frac{1}{4}$         |\n",
    "| 110, 101, 011          | $\\frac{3}{64}$         | $\\frac{9}{64}$         | $\\frac{3}{16}$  | $\\frac{1}{4}$         | $\\frac{3}{4}$         |\n",
    "| 111                    | $\\frac{1}{64}$         | $\\frac{27}{64}$        | $\\frac{7}{16}$  | $\\frac{1}{28}$         | $\\frac{27}{28}$        |  -->\n",
    "\n",
    "| Message $m$   | $\\mathbb{P}(m \\mid c=000)$ | $\\mathbb{P}(m \\mid c=111)$  | Most likely codeword $c$ |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | $\\frac{27}{64}$        | $\\frac{1}{64}$         | 000 <!-- with probability $\\frac{27}{28}$ --> |\n",
    "| 100, 010, 001          | $\\frac{9}{64}$         | $\\frac{3}{64}$         | 000 <!-- with probability $\\frac{3}{4}$ --> |\n",
    "| 110, 101, 011          | $\\frac{3}{64}$         | $\\frac{9}{64}$         | 111 <!-- with probability $\\frac{3}{4}$ --> |\n",
    "| 111                    | $\\frac{1}{64}$         | $\\frac{27}{64}$        | 111 <!-- with probability $\\frac{27}{28}$ --> |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be825a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearcodes import Codeword, LinearCode, HammingCode\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def send_codeword(\n",
    "        code_length: int,\n",
    "        channel_probabilities: list[float],\n",
    "        codeword: Codeword):\n",
    "    if len(channel_probabilities) != code_length:\n",
    "        raise ValueError(f\"List of probabilities must be of length code_length={code_length}. Got {len(channel_probabilities)}.\")\n",
    "    if len(codeword) != code_length:\n",
    "        raise ValueError(f\"Codeword must of length code_length={code_length}. Got {len(codeword)}.\")\n",
    "    message = []\n",
    "    for bit_idx in range(code_length):\n",
    "        p = channel_probabilities[bit_idx]\n",
    "        c = codeword.bits[bit_idx]\n",
    "        r = np.random.uniform(0,1)\n",
    "        if r < p:\n",
    "            flip_bit = (c+1)%2\n",
    "            message.append(flip_bit)\n",
    "        else:\n",
    "            message.append(c)\n",
    "    return message\n",
    "\n",
    "def maximum_likelihood_decoder(channel_probabilities: list[float],\n",
    "                               code: LinearCode, \n",
    "                               message: list[int]) -> Codeword:\n",
    "    \"\"\"\n",
    "    Returns the codeword that was most likely to be sent given message is received.\n",
    "    Args:\n",
    "        - channel_probabilities (list[float]):\n",
    "            entry with index i is the probability bit i will flip.\n",
    "        - code (lc.LinearCode):\n",
    "            code used for encoding.\n",
    "        - message (list[int]):\n",
    "            received message.\n",
    "    \"\"\"\n",
    "    n = code.length\n",
    "    if len(channel_probabilities) != n:\n",
    "        raise ValueError(f\"List of probabilities must be of length {n}. Got {len(channel_probabilities)}.\")\n",
    "    if len(message) != n:\n",
    "        raise ValueError(f\"Message must be of length {n}. Got {len(message)}.\")\n",
    "    codewords = code.codewords\n",
    "    most_likely_codeword = Codeword([0]*n)\n",
    "    most_likely_codeword_prob = 0\n",
    "    for c in codewords:       \n",
    "        c_probability = 1\n",
    "        for bit_idx in range(n):\n",
    "            p = channel_probabilities[bit_idx]\n",
    "            if c.bits[bit_idx] != message[bit_idx]:\n",
    "                c_probability *= p\n",
    "            else:\n",
    "                c_probability *= (1-p)\n",
    "        if c_probability > most_likely_codeword_prob:\n",
    "            most_likely_codeword = c\n",
    "            most_likely_codeword_prob = c_probability\n",
    "    return most_likely_codeword\n",
    "\n",
    "code = HammingCode(3)\n",
    "code_length = code.length\n",
    "codewords = code.codewords\n",
    "channel_probabilities = [float(0.1)]*code_length\n",
    "results: dict[int, tuple[bool, float]] = {}\n",
    "\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "    print(f\"\\n=== Maximimum likelihood decoder {trial+1} ===\")\n",
    "    codeword = random.choice(codewords)\n",
    "\n",
    "    message = send_codeword(code_length, channel_probabilities, codeword)\n",
    "    start_time = time.time()\n",
    "    most_likely_codeword = maximum_likelihood_decoder(channel_probabilities, code, message)\n",
    "    decode_time = time.time() - start_time\n",
    "\n",
    "    # print(f\"\\nRows are: sent codeword c, received message m, most likely sent codeword c'.\")\n",
    "    data = np.array([codeword.bits, message, most_likely_codeword.bits])\n",
    "    # print(data)\n",
    "\n",
    "    decode_correct = codeword == most_likely_codeword\n",
    "    # print(f\"\\nDecoding correct? {decode_correct}\")\n",
    "    # print(f\"Actual number of errors: {sum(1 for bit_idx in range(code_length) if codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "    # print(f\"Guessed number of errors: {sum(1 for bit_idx in range(code_length) if most_likely_codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "\n",
    "    results[trial] = (decode_correct, decode_time)\n",
    "    print(f\"Decode correct? {decode_correct}\")\n",
    "    print(f\"Time to decode = {decode_time*(10**5):.3f}x10^5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09523f",
   "metadata": {},
   "source": [
    "#### Hamming distance\n",
    "\n",
    "We can simplify the Maximum Likelihood Decoder by using the Hamming distance. Recall maximising $\\mathbb{P}(c \\mid m)$ is equivalent to maximising $\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i)$. Let us assume we have a binary symmetric channel such that the probability the probability a bit flips is $p$. Therefore, we have that\n",
    "$$\n",
    "\\mathbb{P}(m_i \\mid c_i) =\n",
    "\\begin{cases}\n",
    "p & \\text{if } m_i \\neq c_i, \\\\\n",
    "1-p & \\text{if } m_i = c_i.\n",
    "\\end{cases}\n",
    "$$\n",
    "Notice that the probabilities depend only whether the bits are the same or different. In particular, the power of $p$ in the product $\\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i)$ is precisely the number of bits in $m$ that differ from the bits in $c$, that is the Hamming distance $d(m,c)$. Therefore, we have that \n",
    "$$\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i) = p^{d(m,c)}(1-p)^{n-d(m,c)}.$$\n",
    "Rearranging this expression we obtain\n",
    "$$\\mathbb{P}(m \\mid c) = \\left( \\frac{p}{1-p}\\right)^{d(m,c)}  \\cdot (1-p)^{n}.$$\n",
    "Note that $(1-p)^{n}$ depends neither on the codeword $c$ nor the message $m$. Thus, maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$ is equivalent to maximising\n",
    "$$\\left( \\frac{p}{1-p}\\right)^{d(m,c)}$$\n",
    "over $\\mathcal{C}$.\n",
    "\n",
    "By convention we assume that $p < \\frac{1}{2}$. If it is not, then we can assume the received message is more likely to be wrong and flip all the bits before working with them .... (Fix this phrasing!). Consequently, the value $\\frac{p}{1-p}$ is less than $1$ and so to maximise $$\\left( \\frac{p}{1-p}\\right)^{d(m,c)}$$ we must minimise $d(m,c)$. In particular, we have that the codeword $\\hat{c}$ that maximises $\\mathbb{P}(c \\mid m)$ is precisely the codeword that minimises $d(m,c)$.\n",
    "\n",
    "---\n",
    "**Example.**\n",
    "\n",
    "Let us consider the example $\\mathcal{C} = \\{000,111\\}$ again. Suppose we receive the message $m=100$. We know from previous computations that the most likely codeword that was sent is $c=000$. We can recompute this using the Hamming distance. In particular, we have that\n",
    "$$ d(m=100,c=000) = 1 \\; \\text{ and } \\; d(m=100,c=111) = 2 $$\n",
    "and so $c=000$ minimises the Hamming distance.\n",
    "\n",
    "As before we can compute the Hamming distance and most likely codeword for all possible messages $m \\in \\{0,1\\}^3$.\n",
    "| Message $m$   | $d(m, c=000)$ | $d(m, c=111)$ | Most likely codeword $c$ |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | 0        | 3         | 000  |\n",
    "| 100, 010, 001          | 1         | 2         | 000  |\n",
    "| 110, 101, 011          | 2         | 1         | 111  |\n",
    "| 111                    | 3         | 0        | 111  |\n",
    "---\n",
    "\n",
    "<!-- \n",
    "\n",
    "Moreover, since the log function is monotonically increasing maximising $\\mathbb{P}(m \\mid c)$ is equivalent to maximising $\\mathrm{log}(\\mathbb{P}(m \\mid c)) $. (TODO: make this a more natural jump.) By applying the log function we obtain\n",
    "$$ \\mathrm{log}(\\mathbb{P}(m \\mid c)) = \\mathrm{log}(\\prod_i \\mathbb{P}(m_i \\mid c_i)) = \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)). $$\n",
    "Now recall that for a binary symmetric channel we assume that \n",
    "In particular, it depends only on when bits are the same and when they are not, similar to the Hamming distance. Thereofore, by using the Hamming distance between two codewords we obtain\n",
    "$$ \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)) = d(c,m)\\mathrm{log}(p) + (1-d(c,m))\\mathrm{log}(1-p). $$\n",
    "Simplifying this we obtain\n",
    "$$ \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)) =  d(c,m)\\mathrm{log}(\\frac{p}{1-p}) + \\mathrm{log}(1-p). $$\n",
    "Notice that $\\mathrm{log}(1-p)$ does not depend on $c$, so maximising $\\mathbb{P}(c \\mid m)$ is equivalent to maximising $$d(c,m)\\mathrm{log}(\\frac{p}{1-p})$$. Since we assume that $p < 0.5$, we have that $\\mathrm{log}(\\frac{p}{1-p})$ is negative. Thus, maximising over $d(c,m)\\mathrm{log}(\\frac{p}{1-p})$ is equivalent to minimsiing over $d(c,m)$. Therefore, we have that\n",
    "$$ \\mathrm{max}_{c \\in \\mathcal{C}} (\\mathbb{P}(c \\mid m)) = \\mathrm{min}_{c \\in \\mathcal{C}} (d(c,m)).$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Take the previous example with the code $\\mathcal{C} = \\{000,111\\}$. We can easily compute the Hamming distances between any message $m$ and codeword $c$ to obtain the following table:\n",
    "\n",
    "| Message $m$   | $d(m, c=000)$ | $d(m, c=111)$ | Closest = most likely codeword |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | 0        | 3         | 000  |\n",
    "| 100, 010, 001          | 1         | 2         | 000  |\n",
    "| 110, 101, 011          | 2         | 1         | 111  |\n",
    "| 111                    | 3         | 0        | 111  | \n",
    "\n",
    "---\n",
    "-->\n",
    "The Hamming distance makes it easier to find the most likely codeword. However, for a code of rank $k$, it still requires computing and comparing $2^k$ different numbers. Therefore, as the code gets bigger the Maximum Likelihood Decoder becomes computationally impractical. We need more efficient algorithms that estimate the most likely codeword without having to compute the Hamming distances explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Minimise Hamming distance decoder 1 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 0 1 0 0 1 0]\n",
      " [1 1 1 0 0 1 0]\n",
      " [1 0 1 0 0 1 0]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 12.827x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 2 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 0 0 0 1 1 1]\n",
      " [0 0 0 0 1 1 1]\n",
      " [0 0 0 0 1 1 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 0\n",
      "Guessed number of errors: 0\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.313x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 3 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 0 1 0 1 0 1]\n",
      " [1 0 1 0 1 1 1]\n",
      " [1 0 1 0 1 0 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.170x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 4 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 1 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 0\n",
      "Guessed number of errors: 0\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.146x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 5 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 1 0 0 1 1]\n",
      " [0 1 1 0 0 1 0]\n",
      " [0 1 1 0 0 1 1]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.217x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 6 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 1 0 1 0 0]\n",
      " [0 1 1 1 1 0 0]\n",
      " [0 1 1 0 1 0 0]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 2.575x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 7 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 0 1 0 1 0]\n",
      " [1 1 1 1 0 1 0]\n",
      " [1 1 1 1 0 0 0]]\n",
      "\n",
      "Decoding correct? False\n",
      "Actual number of errors: 2\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? False\n",
      "Time to decode = 2.480x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 8 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 0 0 0 1 1 1]\n",
      " [0 0 1 1 1 0 1]\n",
      " [0 0 1 1 0 0 1]]\n",
      "\n",
      "Decoding correct? False\n",
      "Actual number of errors: 3\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? False\n",
      "Time to decode = 2.837x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 9 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[0 1 0 1 0 1 0]\n",
      " [0 0 0 1 0 1 1]\n",
      " [1 0 0 1 0 1 1]]\n",
      "\n",
      "Decoding correct? False\n",
      "Actual number of errors: 2\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? False\n",
      "Time to decode = 1.836x10^5\n",
      "\n",
      "=== Minimise Hamming distance decoder 10 ===\n",
      "\n",
      "Rows are: sent codeword c, received message m, most likely sent codeword c'.\n",
      "[[1 0 0 1 1 0 0]\n",
      " [1 1 0 1 1 0 0]\n",
      " [1 0 0 1 1 0 0]]\n",
      "\n",
      "Decoding correct? True\n",
      "Actual number of errors: 1\n",
      "Guessed number of errors: 1\n",
      "\n",
      "Decode correct? True\n",
      "Time to decode = 1.812x10^5\n"
     ]
    }
   ],
   "source": [
    "from linearcodes import Codeword, LinearCode, HammingCode\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def send_codeword(\n",
    "        code_length: int,\n",
    "        channel_probabilities: list[float],\n",
    "        codeword: Codeword):\n",
    "    if len(channel_probabilities) != code_length:\n",
    "        raise ValueError(f\"List of probabilities must be of length code_length={code_length}. Got {len(channel_probabilities)}.\")\n",
    "    if len(codeword) != code_length:\n",
    "        raise ValueError(f\"Codeword must of length code_length={code_length}. Got {len(codeword)}.\")\n",
    "    message = []\n",
    "    for bit_idx in range(code_length):\n",
    "        p = channel_probabilities[bit_idx]\n",
    "        c = codeword.bits[bit_idx]\n",
    "        r = np.random.uniform(0,1)\n",
    "        if r < p:\n",
    "            flip_bit = (c+1)%2\n",
    "            message.append(flip_bit)\n",
    "        else:\n",
    "            message.append(c)\n",
    "    return message\n",
    "\n",
    "def hamming_distance(x: list[int], y: list[int]) -> int:\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both x and y must be the same length.\")\n",
    "    return sum([1 for bit_idx in range(len(x)) if x[bit_idx] != y[bit_idx]])\n",
    "\n",
    "\n",
    "def minimise_hamming_distance(code_length: int,\n",
    "                              codewords: list[Codeword],\n",
    "                              message: list[int]) -> Codeword:\n",
    "    \"\"\"\n",
    "    Returns the codeword that was most likely to be sent given message is received using Hamming distance.\n",
    "    Args:\n",
    "        - channel_probabilities (list[float]):\n",
    "            entry with index i is the probability bit i will flip.\n",
    "        - code (lc.LinearCode):\n",
    "            code used for encoding.\n",
    "        - message (list[int]):\n",
    "            received message.\n",
    "    \"\"\"\n",
    "    if len(message) != code_length:\n",
    "        raise ValueError(f\"Message must be of length {code_length}. Got {len(message)}.\")\n",
    "    most_likely_codeword = Codeword([0]*code_length)\n",
    "    min_hamming_dist = code_length\n",
    "    for c in codewords:       \n",
    "        hamming_dist = hamming_distance(c.bits, message)\n",
    "        if hamming_dist < min_hamming_dist:\n",
    "            most_likely_codeword = c\n",
    "            min_hamming_dist = hamming_dist\n",
    "    return most_likely_codeword\n",
    "\n",
    "code = HammingCode(3)\n",
    "code_length = code.length\n",
    "codewords = code.codewords\n",
    "channel_probabilities = [float(0.1)]*code_length\n",
    "results: dict[int, tuple[bool, float]] = {}\n",
    "\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "    print(f\"\\n=== Minimise Hamming distance decoder {trial+1} ===\")\n",
    "    codeword = random.choice(codewords)\n",
    "\n",
    "    message = send_codeword(code_length, channel_probabilities, codeword)\n",
    "    start_time = time.time()\n",
    "    most_likely_codeword = minimise_hamming_distance(code_length, codewords, message)\n",
    "    decode_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nRows are: sent codeword c, received message m, most likely sent codeword c'.\")\n",
    "    data = np.array([codeword.bits, message, most_likely_codeword.bits])\n",
    "    print(data)\n",
    "\n",
    "    decode_correct = codeword == most_likely_codeword\n",
    "    print(f\"\\nDecoding correct? {decode_correct}\")\n",
    "    print(f\"Actual number of errors: {sum(1 for bit_idx in range(code_length) if codeword.bits[bit_idx] != message[bit_idx])}\")\n",
    "    print(f\"Guessed number of errors: {sum(1 for bit_idx in range(code_length) if most_likely_codeword.bits[bit_idx] != message[bit_idx])}\")\n",
    "\n",
    "    results[trial] = (decode_correct, decode_time)\n",
    "    print(f\"\\nDecode correct? {decode_correct}\")\n",
    "    print(f\"Time to decode = {decode_time*(10**5):.3f}x10^5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
