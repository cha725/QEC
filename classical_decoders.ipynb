{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4f4683",
   "metadata": {},
   "source": [
    "### Maximum likelihood decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a7380",
   "metadata": {},
   "source": [
    "When a codeword is sent through a physical channel, some bits may be corrupted. As a result, the received message might differ from the original codeword. Decoding aims to recover the original codeword despite these errors. Without knowing the exact errors that occurred, we cannot be certain that decoding will find the intended message. So instead we try to find the codeword that is most likely to have been sent. More precisely, let $\\mathcal{C}$ be a code of length $n \\in \\mathbb{Z}_{>0}$ and suppose we receive a message $m \\in \\{0,1\\}^n$. Then we wish to find a codeword $\\hat{c} \\in \\mathcal{C}$ such that \n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(\\text{ Sent } \\hat{c} \\mid \\text{ Received } m) = \\mathrm{max}_{c \\in \\mathcal{C}} \\{\\mathbb{P}(\\text{ Sent } c \\mid \\text{ Received } m)\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "We denote $\\mathbb{P}(\\text{ Sent } c \\mid \\text{ Received } m)$ by  $\\mathbb{P}(c \\mid m)$. \n",
    "The application of Bayes Theorem to $\\mathbb{P}(c \\mid m)$ results in\n",
    "$$\\mathbb{P}(c \\mid m) = \\frac{\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)}{\\mathbb{P}(m)}.$$\n",
    "Notice that $\\mathbb{P}(m)$ does not depend on the specific codeword $c$, and so, to maximise $\\mathbb{P}(c \\mid m)$ over the code $\\mathcal{C}$ it suffices to maximise $$\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)$$ over $\\mathcal{C}$. To do this, we first make a couple of assumptions:\n",
    "\n",
    "1. **Uniform codeword distribution:** Each codeword is equally likely to be sent. That is, for all codewords $c \\in \\mathcal{C}$ we have that $$\\mathbb{P}(c) = \\frac{1}{|\\mathcal{C}|}.$$ Thus, maximising $\\mathbb{P}(m \\mid c)\\cdot \\mathbb{P}(c)$ over $\\mathcal{C}$ is equivalent to maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$.\n",
    "\n",
    "2. **All errors are bit-flips:** A codeword $c \\in \\mathcal{C}$ produces a message $m$ that is a binary string of the same length. \n",
    "\n",
    "3. **Bit-flips are independent:** Each bit flips independently of others. That is, for a message $m=m_1m_2\\dots m_n$ and a codeword $c=c_1c_2\\dots c_n$, we have that\n",
    "    $$\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i).$$\n",
    "    In particular, since both $m_i$ and $c_i$ are bits, we have that:\n",
    "    $$ \\mathbb{P}(m_i \\mid c_i) = \\begin{cases} \\mathbb{P}(c_i \\text{ not flipped}) &\\text{ if } m_i=c_i \\\\ \\mathbb{P}(c_i \\text{ flipped to } m_i) &\\text{ if } m_i \\neq c_i. \\end{cases} $$\n",
    "\n",
    "Therefore, to compute $\\mathbb{P}(c \\mid m)$ for any codeword $c$ and message $m$ of length $n \\in \\mathbb{Z}_{>0}$, it suffices to know for each $1 \\leq i \\leq n$, the probabilities\n",
    "$$ \\mathbb{P}(c_i \\text{ flips } 0 \\to 1) \\; \\text{ and } \\; \\mathbb{P}(c_i \\text{ flips } 1 \\to 0). $$\n",
    "A _binary symmetric channel_ is a channel where these two probabilities coincide and a bit flips with the same probability from $0$ to $1$ as it does from $1$ to $0$.\n",
    "\n",
    "---\n",
    "##### Example.\n",
    "\n",
    "Let us consider the code $\\mathcal{C} = \\{000, 111\\}$. Suppose that we use a binary symmetric channel and each bit has a probability $p=\\frac{1}{4}$ of flipping regardless of position. Suppose that we receive the message $m=100$ and we wish to compute the most likely codeword it came from. Since maximising $\\mathbb{P}(c \\mid m)$ over $\\mathcal{C}$ is equivalent to maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$, it suffices to compute $\\mathbb{P}(m \\mid c)$ for each codeword $c \\in \\mathcal{C}$. That is, for $c=000$ and $c=111$.\n",
    "\n",
    "First suppose the codeword $c$ was $000$. Then receiving the message $m=100$ means the first bit flipped and the other two did not flip (TODO: can a bit flip more than once?). In particular, we have\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}(m=100 \\mid c=000) &= \\mathbb{P}(m_1=1 \\mid c_1=0) \\cdot \\mathbb{P}(m_2=0 \\mid c_2=0) \\cdot \\mathbb{P}(m_3=0 \\mid c_3=0) \\\\ &= \\mathbb{P}(c_1 \\text{ did flip}) \\cdot \\mathbb{P}(c_2 \\text{ did not flip}) \\cdot \\mathbb{P}(c_3 \\text{ did not flip}) \\\\ &= \\frac{1}{4} \\cdot \\frac{3}{4} \\cdot \\frac{3}{4} = \\frac{9}{64}. \n",
    "\\end{align*}\n",
    "Now suppose the codeword $c$ was $111$, then receiving the message $m=100$ means the first bit did not flip and the other two did flip, so we have\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(m=100 \\mid c=111) = \\frac{3}{4} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} = \\frac{3}{64}. \n",
    "\\end{equation*}\n",
    "Therefore, as $\\mathbb{P}(m=100 \\mid c=000)$ is larger than $\\mathbb{P}(m=100 \\mid c=111)$ it is most likely that the message $m=100$ came from the codeword $c=000$.\n",
    "<!-- \n",
    "Therefore, we have that\n",
    "\\begin{align*}\n",
    "    \\sum_{b \\in \\mathcal{C}} \\mathbb{P}(m \\mid b) &= P(m \\mid c=000) + P(m \\mid c=111) \n",
    "    \\\\\n",
    "    &= \\frac{9}{64} + \\frac{3}{64} = \\frac{3}{16}.\n",
    "\\end{align*}\n",
    "Thus, substituting these probabilities in, it follows that\n",
    "$$\\mathbb{P}(c=000 \\mid m) = \\frac{1}{|\\mathcal{C}|} \\cdot \\frac{\\mathbb{P}(m \\mid c=000)}{\\sum_{d \\in \\mathcal{C}} \\mathbb{P}(m \\mid d)} =  \\frac{1}{1}\\cdot \\frac{\\frac{9}{64}}{\\frac{3}{16}} = \\frac{3}{4},$$\n",
    "and\n",
    "$$\\mathbb{P}(c=111 \\mid m) = \\frac{1}{|\\mathcal{C}|} \\cdot \\frac{\\mathbb{P}(m \\mid c=111)}{\\sum_{d \\in \\mathcal{C}} \\mathbb{P}(m \\mid d)} =  \\frac{1}{1}\\cdot \\frac{\\frac{3}{64}}{\\frac{3}{16}} = \\frac{1}{4}.$$\n",
    "Therefore, if $m=100$ is the transmitted message, then the most likely codeword that was sent is $c=000$ with a probability of $\\frac{3}{4}$. -->\n",
    "\n",
    "We can compute the most likely codeword for each of the possible messages $m \\in (\\mathbb{Z}/2\\mathbb{Z})^3$.\n",
    "\n",
    "<!-- | Message $m$   | $\\mathbb{P}(m \\mid c=000)$ | $\\mathbb{P}(m \\mid c=111)$ | $\\sum_{b \\in \\mathcal{C}} \\mathbb{P}(m \\mid b)$ | $\\mathbb{P}(c=000 \\mid m)$ | $\\mathbb{P}(c=111 \\mid m)$ |\n",
    "| ---                    | ---                    | ---                    | ---             |---                    | ---                    |\n",
    "| 000                    | $\\frac{27}{64}$        | $\\frac{1}{64}$         | $\\frac{7}{16}$  | $\\frac{27}{28}$        | $\\frac{1}{28}$         |\n",
    "| 100, 010, 001          | $\\frac{9}{64}$         | $\\frac{3}{64}$         | $\\frac{3}{16}$  | $\\frac{3}{4}$         | $\\frac{1}{4}$         |\n",
    "| 110, 101, 011          | $\\frac{3}{64}$         | $\\frac{9}{64}$         | $\\frac{3}{16}$  | $\\frac{1}{4}$         | $\\frac{3}{4}$         |\n",
    "| 111                    | $\\frac{1}{64}$         | $\\frac{27}{64}$        | $\\frac{7}{16}$  | $\\frac{1}{28}$         | $\\frac{27}{28}$        |  -->\n",
    "\n",
    "| Message $m$   | $\\mathbb{P}(m \\mid c=000)$ | $\\mathbb{P}(m \\mid c=111)$  | Most likely codeword $c$ |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | $\\frac{27}{64}$        | $\\frac{1}{64}$         | 000 <!-- with probability $\\frac{27}{28}$ --> |\n",
    "| 100, 010, 001          | $\\frac{9}{64}$         | $\\frac{3}{64}$         | 000 <!-- with probability $\\frac{3}{4}$ --> |\n",
    "| 110, 101, 011          | $\\frac{3}{64}$         | $\\frac{9}{64}$         | 111 <!-- with probability $\\frac{3}{4}$ --> |\n",
    "| 111                    | $\\frac{1}{64}$         | $\\frac{27}{64}$        | 111 <!-- with probability $\\frac{27}{28}$ --> |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be825a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearcodes import Codeword, LinearCode, HammingCode\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def send_codeword(\n",
    "        code_length: int,\n",
    "        channel_probabilities: list[float],\n",
    "        codeword: Codeword):\n",
    "    if len(channel_probabilities) != code_length:\n",
    "        raise ValueError(f\"List of probabilities must be of length code_length={code_length}. Got {len(channel_probabilities)}.\")\n",
    "    if len(codeword) != code_length:\n",
    "        raise ValueError(f\"Codeword must of length code_length={code_length}. Got {len(codeword)}.\")\n",
    "    message = []\n",
    "    for bit_idx in range(code_length):\n",
    "        p = channel_probabilities[bit_idx]\n",
    "        c = codeword.bits[bit_idx]\n",
    "        r = np.random.uniform(0,1)\n",
    "        if r < p:\n",
    "            flip_bit = (c+1)%2\n",
    "            message.append(flip_bit)\n",
    "        else:\n",
    "            message.append(c)\n",
    "    return message\n",
    "\n",
    "def maximum_likelihood_decoder(channel_probabilities: list[float],\n",
    "                               code: LinearCode, \n",
    "                               message: list[int]) -> Codeword:\n",
    "    \"\"\"\n",
    "    Returns the codeword that was most likely to be sent given message is received.\n",
    "    Args:\n",
    "        - channel_probabilities (list[float]):\n",
    "            entry with index i is the probability bit i will flip.\n",
    "        - code (lc.LinearCode):\n",
    "            code used for encoding.\n",
    "        - message (list[int]):\n",
    "            received message.\n",
    "    \"\"\"\n",
    "    n = code.length\n",
    "    if len(channel_probabilities) != n:\n",
    "        raise ValueError(f\"List of probabilities must be of length {n}. Got {len(channel_probabilities)}.\")\n",
    "    if len(message) != n:\n",
    "        raise ValueError(f\"Message must be of length {n}. Got {len(message)}.\")\n",
    "    codewords = code.codewords\n",
    "    most_likely_codeword = Codeword([0]*n)\n",
    "    most_likely_codeword_prob = 0\n",
    "    for c in codewords:       \n",
    "        c_probability = 1\n",
    "        for bit_idx in range(n):\n",
    "            p = channel_probabilities[bit_idx]\n",
    "            if c.bits[bit_idx] != message[bit_idx]:\n",
    "                c_probability *= p\n",
    "            else:\n",
    "                c_probability *= (1-p)\n",
    "        if c_probability > most_likely_codeword_prob:\n",
    "            most_likely_codeword = c\n",
    "            most_likely_codeword_prob = c_probability\n",
    "    return most_likely_codeword\n",
    "\n",
    "code = HammingCode(3)\n",
    "code_length = code.length\n",
    "codewords = code.codewords\n",
    "channel_probabilities = [float(0.1)]*code_length\n",
    "results: dict[int, tuple[bool, float]] = {}\n",
    "\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "    print(f\"\\n=== Maximimum likelihood decoder {trial+1} ===\")\n",
    "    codeword = random.choice(codewords)\n",
    "\n",
    "    message = send_codeword(code_length, channel_probabilities, codeword)\n",
    "    start_time = time.time()\n",
    "    most_likely_codeword = maximum_likelihood_decoder(channel_probabilities, code, message)\n",
    "    decode_time = time.time() - start_time\n",
    "\n",
    "    # print(f\"\\nRows are: sent codeword c, received message m, most likely sent codeword c'.\")\n",
    "    data = np.array([codeword.bits, message, most_likely_codeword.bits])\n",
    "    # print(data)\n",
    "\n",
    "    decode_correct = codeword == most_likely_codeword\n",
    "    # print(f\"\\nDecoding correct? {decode_correct}\")\n",
    "    # print(f\"Actual number of errors: {sum(1 for bit_idx in range(code_length) if codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "    # print(f\"Guessed number of errors: {sum(1 for bit_idx in range(code_length) if most_likely_codeword.vector[bit_idx] != message[bit_idx])}\")\n",
    "\n",
    "    results[trial] = (decode_correct, decode_time)\n",
    "    print(f\"Decode correct? {decode_correct}\")\n",
    "    print(f\"Time to decode = {decode_time*(10**5):.3f}x10^5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09523f",
   "metadata": {},
   "source": [
    "#### Hamming distance\n",
    "\n",
    "We can simplify the Maximum Likelihood Decoder by using the Hamming distance. Recall maximising $\\mathbb{P}(c \\mid m)$ is equivalent to maximising $\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i)$. Let us assume we have a binary symmetric channel such that the probability the probability a bit flips is $p$. Therefore, we have that\n",
    "$$\n",
    "\\mathbb{P}(m_i \\mid c_i) =\n",
    "\\begin{cases}\n",
    "p & \\text{if } m_i \\neq c_i, \\\\\n",
    "1-p & \\text{if } m_i = c_i.\n",
    "\\end{cases}\n",
    "$$\n",
    "Notice that the probabilities depend only whether the bits are the same or different. In particular, the power of $p$ in the product $\\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i)$ is precisely the number of bits in $m$ that differ from the bits in $c$, that is the Hamming distance $d(m,c)$. Therefore, we have that \n",
    "$$\\mathbb{P}(m \\mid c) = \\prod_{i=1}^n \\mathbb{P}(m_i \\mid c_i) = p^{d(m,c)}(1-p)^{n-d(m,c)}.$$\n",
    "Rearranging this expression we obtain\n",
    "$$\\mathbb{P}(m \\mid c) = \\left( \\frac{p}{1-p}\\right)^{d(m,c)}  \\cdot (1-p)^{n}.$$\n",
    "Note that $(1-p)^{n}$ depends neither on the codeword $c$ nor the message $m$. Thus, maximising $\\mathbb{P}(m \\mid c)$ over $\\mathcal{C}$ is equivalent to maximising\n",
    "$$\\left( \\frac{p}{1-p}\\right)^{d(m,c)}$$\n",
    "over $\\mathcal{C}$.\n",
    "\n",
    "By convention we assume that $p < \\frac{1}{2}$. If it is not, then we can assume the received message is more likely to be wrong and flip all the bits before working with them .... (Fix this phrasing!). Consequently, the value $\\frac{p}{1-p}$ is less than $1$ and so to maximise $$\\left( \\frac{p}{1-p}\\right)^{d(m,c)}$$ we must minimise $d(m,c)$. In particular, we have that the codeword $\\hat{c}$ that maximises $\\mathbb{P}(c \\mid m)$ is precisely the codeword that minimises $d(m,c)$.\n",
    "\n",
    "---\n",
    "**Example.**\n",
    "\n",
    "Let us consider the example $\\mathcal{C} = \\{000,111\\}$ again. Suppose we receive the message $m=100$. We know from previous computations that the most likely codeword that was sent is $c=000$. We can recompute this using the Hamming distance. In particular, we have that\n",
    "$$ d(m=100,c=000) = 1 \\; \\text{ and } \\; d(m=100,c=111) = 2 $$\n",
    "and so $c=000$ minimises the Hamming distance.\n",
    "\n",
    "As before we can compute the Hamming distance and most likely codeword for all possible messages $m \\in \\{0,1\\}^3$.\n",
    "| Message $m$   | $d(m, c=000)$ | $d(m, c=111)$ | Most likely codeword $c$ |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | 0        | 3         | 000  |\n",
    "| 100, 010, 001          | 1         | 2         | 000  |\n",
    "| 110, 101, 011          | 2         | 1         | 111  |\n",
    "| 111                    | 3         | 0        | 111  |\n",
    "---\n",
    "\n",
    "<!-- \n",
    "\n",
    "Moreover, since the log function is monotonically increasing maximising $\\mathbb{P}(m \\mid c)$ is equivalent to maximising $\\mathrm{log}(\\mathbb{P}(m \\mid c)) $. (TODO: make this a more natural jump.) By applying the log function we obtain\n",
    "$$ \\mathrm{log}(\\mathbb{P}(m \\mid c)) = \\mathrm{log}(\\prod_i \\mathbb{P}(m_i \\mid c_i)) = \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)). $$\n",
    "Now recall that for a binary symmetric channel we assume that \n",
    "In particular, it depends only on when bits are the same and when they are not, similar to the Hamming distance. Thereofore, by using the Hamming distance between two codewords we obtain\n",
    "$$ \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)) = d(c,m)\\mathrm{log}(p) + (1-d(c,m))\\mathrm{log}(1-p). $$\n",
    "Simplifying this we obtain\n",
    "$$ \\sum_i \\mathrm{log}(\\mathbb{P}(m_i \\mid c_i)) =  d(c,m)\\mathrm{log}(\\frac{p}{1-p}) + \\mathrm{log}(1-p). $$\n",
    "Notice that $\\mathrm{log}(1-p)$ does not depend on $c$, so maximising $\\mathbb{P}(c \\mid m)$ is equivalent to maximising $$d(c,m)\\mathrm{log}(\\frac{p}{1-p})$$. Since we assume that $p < 0.5$, we have that $\\mathrm{log}(\\frac{p}{1-p})$ is negative. Thus, maximising over $d(c,m)\\mathrm{log}(\\frac{p}{1-p})$ is equivalent to minimsiing over $d(c,m)$. Therefore, we have that\n",
    "$$ \\mathrm{max}_{c \\in \\mathcal{C}} (\\mathbb{P}(c \\mid m)) = \\mathrm{min}_{c \\in \\mathcal{C}} (d(c,m)).$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Take the previous example with the code $\\mathcal{C} = \\{000,111\\}$. We can easily compute the Hamming distances between any message $m$ and codeword $c$ to obtain the following table:\n",
    "\n",
    "| Message $m$   | $d(m, c=000)$ | $d(m, c=111)$ | Closest = most likely codeword |\n",
    "| ---                    | ---                    | ---                    | ---             |\n",
    "| 000                    | 0        | 3         | 000  |\n",
    "| 100, 010, 001          | 1         | 2         | 000  |\n",
    "| 110, 101, 011          | 2         | 1         | 111  |\n",
    "| 111                    | 3         | 0        | 111  | \n",
    "\n",
    "---\n",
    "-->\n",
    "The Hamming distance makes it easier to find the most likely codeword. However, for a code of rank $k$, it still requires computing and comparing $2^k$ different numbers. Therefore, as the code gets bigger the Maximum Likelihood Decoder becomes computationally impractical. We need more efficient algorithms that estimate the most likely codeword without having to compute the Hamming distances explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearcodes import Codeword, LinearCode, HammingCode\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def send_codeword(\n",
    "        code_length: int,\n",
    "        channel_probabilities: list[float],\n",
    "        codeword: Codeword):\n",
    "    if len(channel_probabilities) != code_length:\n",
    "        raise ValueError(f\"List of probabilities must be of length code_length={code_length}. Got {len(channel_probabilities)}.\")\n",
    "    if len(codeword) != code_length:\n",
    "        raise ValueError(f\"Codeword must of length code_length={code_length}. Got {len(codeword)}.\")\n",
    "    message = []\n",
    "    for bit_idx in range(code_length):\n",
    "        p = channel_probabilities[bit_idx]\n",
    "        c = codeword.bits[bit_idx]\n",
    "        r = np.random.uniform(0,1)\n",
    "        if r < p:\n",
    "            flip_bit = (c+1)%2\n",
    "            message.append(flip_bit)\n",
    "        else:\n",
    "            message.append(c)\n",
    "    return message\n",
    "\n",
    "def hamming_distance(x: list[int], y: list[int]) -> int:\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both x and y must be the same length.\")\n",
    "    return sum([1 for bit_idx in range(len(x)) if x[bit_idx] != y[bit_idx]])\n",
    "\n",
    "\n",
    "def minimise_hamming_distance(code_length: int,\n",
    "                              codewords: list[Codeword],\n",
    "                              message: list[int]) -> Codeword:\n",
    "    \"\"\"\n",
    "    Returns the codeword that was most likely to be sent given message is received using Hamming distance.\n",
    "    Args:\n",
    "        - channel_probabilities (list[float]):\n",
    "            entry with index i is the probability bit i will flip.\n",
    "        - code (lc.LinearCode):\n",
    "            code used for encoding.\n",
    "        - message (list[int]):\n",
    "            received message.\n",
    "    \"\"\"\n",
    "    if len(message) != code_length:\n",
    "        raise ValueError(f\"Message must be of length {code_length}. Got {len(message)}.\")\n",
    "    most_likely_codeword = Codeword([0]*code_length)\n",
    "    min_hamming_dist = code_length\n",
    "    for c in codewords:       \n",
    "        hamming_dist = hamming_distance(c.bits, message)\n",
    "        if hamming_dist < min_hamming_dist:\n",
    "            most_likely_codeword = c\n",
    "            min_hamming_dist = hamming_dist\n",
    "    return most_likely_codeword\n",
    "\n",
    "code = HammingCode(3)\n",
    "code_length = code.length\n",
    "codewords = code.codewords\n",
    "channel_probabilities = [float(0.1)]*code_length\n",
    "results: dict[int, tuple[bool, float]] = {}\n",
    "\n",
    "num_trials = 10\n",
    "for trial in range(num_trials):\n",
    "    print(f\"\\n=== Minimise Hamming distance decoder {trial+1} ===\")\n",
    "    codeword = random.choice(codewords)\n",
    "\n",
    "    message = send_codeword(code_length, channel_probabilities, codeword)\n",
    "    start_time = time.time()\n",
    "    most_likely_codeword = minimise_hamming_distance(code_length, codewords, message)\n",
    "    decode_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nRows are: sent codeword c, received message m, most likely sent codeword c'.\")\n",
    "    data = np.array([codeword.bits, message, most_likely_codeword.bits])\n",
    "    print(data)\n",
    "\n",
    "    decode_correct = codeword == most_likely_codeword\n",
    "    print(f\"\\nDecoding correct? {decode_correct}\")\n",
    "    print(f\"Actual number of errors: {sum(1 for bit_idx in range(code_length) if codeword.bits[bit_idx] != message[bit_idx])}\")\n",
    "    print(f\"Guessed number of errors: {sum(1 for bit_idx in range(code_length) if most_likely_codeword.bits[bit_idx] != message[bit_idx])}\")\n",
    "\n",
    "    results[trial] = (decode_correct, decode_time)\n",
    "    print(f\"\\nDecode correct? {decode_correct}\")\n",
    "    print(f\"Time to decode = {decode_time*(10**5):.3f}x10^5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ce50c",
   "metadata": {},
   "source": [
    "## Message Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510d6df",
   "metadata": {},
   "source": [
    "Maximum likelihood decoding identifies the most likely codeword for a received message by comparing it against every possible codeword. While this is theoretically optimal, it becomes infeasible for practical codes because the number of codewords grows exponentially with the code rank. Message passing is a scalable alternative. It is a graph-based algorithm where each node has local information and can communicate only with its immediate neighbours. Nodes iteratively update their knowledge based on messages received, and through repeated exchanges, information spreads across the network. Over time, global properties can be inferred even though no node initially has complete knowledge.\n",
    "\n",
    "Message passing is a analogy of social networks. People who spend a lot of time together tend to share opinions and information. Each individual updates their understanding based on what they hear from friends. Then when they passes this updated information along to other friends. As this process iterates, ideas and knowledge propagate through the network, allowing people to build a collective understanding even without direct access to everyoneâ€™s thoughts.\n",
    "\n",
    "We can use this type of algorithm on the Tanner graph of an error correcting code. Bit nodes and parity check nodes will exchange messages along edges of the graph, updating their beliefs about errors iteratively. First we do a concrete example of message passing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e53d66",
   "metadata": {},
   "source": [
    "### Think of a title\n",
    "\n",
    "Suppose we wish to count the number of vertices in a tree. Each node can only see itself and its neighbours, so it only has local information. However, we can use message passing to combine this information and create a global picture.\n",
    "\n",
    "Let $V$ be the set of vertices and $E$ be the set of edges of a graph $G=(V, E)$. For a vertex $u \\in V$ and let $\\mathcal{N}(u)$ denote the neighbourhood of the vertex $u$ in $G$. For each vertex $u \\in V$, we define a sequence integers $$N_0(u), \\, N_1(u), \\, N_2(u), \\, N_3(u) \\, \\dots$$ by the following algorithm:\n",
    "\n",
    "**Initialisation:** For each vertex $u \\in V$, define $N_0(u) = 1$. For each pair of neighbouring vertices $(u,v)$ define $m_1(u,v) = 1$.\n",
    "\n",
    "**Iterative step for $i>0$:**\n",
    "1. **Update vertex values:** Update the information for each vertex using the following formula\n",
    "$$N_i(u) = N_{i-1}(u) + \\sum_{v \\in \\mathcal{N}(u)} m_{i}(v,u).$$\n",
    "2. **Update messages:** For each pair of neighbouring vertices $(u,v)$ define a message $m_{i+1}(u,v)$ by\n",
    "    $$ m_{i+1}(u,v) = \\sum_{\\substack{w \\in \\mathcal{N}(u) \\\\ w \\neq v}} m_{i}(w,u). $$\n",
    "\n",
    "    The message $m_{i+1}(u,v)$ will be the information \"sent\" from vertex $u$ to vertex $v$. This is why we do not include $v$ in the summation index for $m_i(u,v)$. If we did we would be sending information back to $v$ that it already knows and sent to us on the previous iteration.\n",
    "3. **Check stop condition:** If all the messages are $0$, stop the algorithm.\n",
    " \n",
    "**Claim:** Let $G = (V,E)$ be a finite tree. Then the algorithm terminates after finitely many steps. Moreover, if the algorithm terminates on iteration $i>0$, then for each vertex $u \\in V$, the value $N_i(u)$ is the number of vertices in $T$.\n",
    "\n",
    "Before, proving this statement, let us consider a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5dcd1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Example.** Consider an $A_3$ graph, that is a straight line with three vertices\n",
    "$$ u_1 \\text{---} u_2 \\text{---} u_3. $$\n",
    "Each of $N_0(u_i)$ is initialised at $1$ and each message is initialised at $1$. Now let us apply the algorithm.\n",
    "\n",
    "**Iteration 1**\n",
    "1. **Update vertex values:** We have that\n",
    "    $$ N_1(u_1) = N_0(u_1) + \\sum_{v \\in \\mathcal{N}(u_1)} m_1(v,u_1) = N_0(u_1) + m_1(u_2, u_1) = 1 + 1 = 2.$$\n",
    "    Similarly, we compute that $N_1(u_2) = 3$ and $N_1(u_3)=1$.\n",
    "2. **Update messages:** Since $u_1$ and $u_3$ have exactly one neighbour, namely $u_2$, we have that\n",
    "    $$m_2(u_1,u_2) = \\sum_{\\substack{w \\in \\mathcal{N}(u_1) \\\\ w \\neq u_2}} m_{1}(w,u_1) = 0$$\n",
    "    and similarly $m_2(u_3,u_2) = 0$. However, we have two non-zero messages given by\n",
    "    $$m_2(u_2,u_1) = \\sum_{\\substack{w \\in \\mathcal{N}(u_2) \\\\ w \\neq u_1}} m_{1}(w,u_2) = m_{1}(u_3,u_2)=1$$\n",
    "    and $m_2(u_2,u_3) = m_1(u_1,u_2) = 1$.\n",
    "3. **Check stop condition:** Two of the messages are non-zero.\n",
    "\n",
    "**Iteration 2**\n",
    "1. **Update vertex values:** Since $u_2$ only receives zero messages, it follows that $N_2(u_2) = N_1(u_2) = 3$. For $u_1$ we have that\n",
    "    $$ N_2(u_1) = N_1(u_1) + \\sum_{v \\in \\mathcal{N}(u_1)} m_2(v,u_1) = N_1(u_1) + m_2(u_2, u_1) = 2 + 1 = 3.$$\n",
    "    Similarly, we compute that $N_2(u_3) = 3$.\n",
    "2. **Define messages:** All the messages will be zero in this iteration. Again since $u_1$ and $u_3$ have exactly one neighbour, namely $u_2$, we have that $m_3(u_1,u_2) = 0$ and $m_3(u_3,u_2)=0$. Moreover, we have that\n",
    "    $$m_3(u_2,u_1) = \\sum_{\\substack{w \\in \\mathcal{N}(u_2) \\\\ w \\neq u_1}} m_{1}(w,u_2) = m_2(u_3,u_2)=0$$\n",
    "    and $m_3(u_2,u_3) = m_2(u_1,u_2) = 0$.\n",
    "3. **Check stop condition:** All the messages are $0$. Stop the algorithm.\n",
    "\n",
    "The algorithm terminated on iteration $2$, so for each vertex we obtained a sequence of $3$ values, as follows\n",
    "$$ u_1: \\, N_0(u_1) = 1, \\, N_1(u_1) = 2, \\, N_2(u_1) = 3, $$\n",
    "$$ u_2: \\, N_0(u_2) = 1, \\, N_1(u_2) = 3, \\, N_2(u_2) = 3, $$\n",
    "$$ u_3: \\, N_0(u_3) = 1, \\, N_1(u_3) = 2, \\, N_2(u_3) = 3. $$\n",
    "The final value of each sequence is $3$; the number of vertices in $A_3$.\n",
    "\n",
    "---"
   ]
  },
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
